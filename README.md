# AIG 230 â€“ Lab 04  
## Word Embeddings: Learning Meaning from Context

Working with:

- **Gensim** for training Word2Vec and FastText models  
- **scikit-learn** for dataset loading and dimensionality reduction  
- **NLTK** for basic tokenization and preprocessing  
- **matplotlib** for visualization  

---

Using the 20Newsgroups corpus, I did a comparison of Word2Vec and FastText using the target word "space" to determine the differences between the top 10 neighbors and their prediction scores given windos of 2 and 10.
I also varied the vector size from 50 to 100 to 200.



