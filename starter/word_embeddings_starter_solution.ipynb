{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2d650af",
      "metadata": {
        "id": "a2d650af"
      },
      "source": [
        "# AIG 230 — Week 4 Lab Notebook  \n",
        "## Word Embeddings: Learning Meaning from Context\n",
        "\n",
        "**Goal of this lab:** learn how *continuous vector representations* (embeddings) are learned from data, and how they are used in practical NLP workflows.\n",
        "\n",
        "### Learning objectives\n",
        "By the end of this lab, you should be able to:\n",
        "1. Explain **distributional semantics** (\"meaning from context\").\n",
        "2. Train word embeddings using **Word2Vec** and **FastText** (with Gensim).\n",
        "3. Use embeddings for **similarity search** and **analogy / vector algebra**.\n",
        "4. Visualize a word embedding space and interpret what you see.\n",
        "5. Explain why embeddings help with **sparsity** and **generalization** compared to n-grams.\n",
        "\n",
        "### Industry relevance (why you should care)\n",
        "Even in the era of transformers, embeddings remain widely used for:\n",
        "- **Semantic search** and query expansion  \n",
        "- **Similarity / recommendation** (nearest neighbors in vector space)  \n",
        "- **Clustering** and exploratory analysis  \n",
        "- Lightweight NLP systems where large models are too expensive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12c01854",
      "metadata": {
        "id": "12c01854"
      },
      "source": [
        "---\n",
        "## Checkpoint 0 (Before you code)\n",
        "In 2 to 4 sentences, answer:\n",
        "\n",
        "1. What is the main limitation of **one-hot vectors**?\n",
        "2. What does the phrase **\"meaning emerges from co-occurrence patterns\"** mean to you?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sparsity (lots of zeros in the vectors) and use of a lot of the vectors and high dimensionality taking up valuable memory. They also fail to capture any semantic similarity between words, because every word is treated as completely independent.\n",
        "2. Meaning emerges from co-occurrence patterns means that when words tend to often appear together, it's easier to infer meaning from either of the words because the presence of one could indicate the presence of the other. Or, the words that appear around a word change the context and therefore the meaning of the word we're focusing on."
      ],
      "metadata": {
        "id": "XzXMlJNgeUly"
      },
      "id": "XzXMlJNgeUly"
    },
    {
      "cell_type": "markdown",
      "id": "fde51e3c",
      "metadata": {
        "id": "fde51e3c"
      },
      "source": [
        "---\n",
        "## Part 0 — Setup\n",
        "\n",
        "We will use:\n",
        "- **scikit-learn** to load a real dataset (20 Newsgroups)\n",
        "- **NLTK** for basic tokenization and stopword removal\n",
        "- **Gensim** to train Word2Vec and FastText models\n",
        "- **matplotlib** for visualization\n",
        "\n",
        "> Tip: If you are running this notebook in Colab, the `pip install` cell below is necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a823e647",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a823e647",
        "outputId": "b01a61ee-4797-46a9-d04d-063825041fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required libraries (run once)\n",
        "!pip -q install gensim scikit-learn nltk matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2e3f7be1",
      "metadata": {
        "id": "2e3f7be1"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec, FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b1856c2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1856c2c",
        "outputId": "ba00d6a4-9cb8-427f-b498-3790985838fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Download NLTK resources (run once per environment)\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52f0d75d",
      "metadata": {
        "id": "52f0d75d"
      },
      "source": [
        "---\n",
        "## Part 1 — Data: A real-world corpus\n",
        "\n",
        "We will train embeddings on the **20 Newsgroups** dataset.  \n",
        "This dataset contains posts from 20 topics (e.g., computers, sports, politics).\n",
        "\n",
        "Why this dataset is useful for embeddings:\n",
        "- It's real text (messy, varied vocabulary)\n",
        "- It contains many topic clusters, which embeddings can capture\n",
        "- It is large enough to learn meaningful co-occurrence patterns\n",
        "\n",
        "\n",
        "https://www.kaggle.com/datasets/crawford/20-newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "68559f3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68559f3a",
        "outputId": "a3404278-5926-4d25-f39c-8ac43df50ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 11314\n",
            "\n",
            "Example document snippet:\n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the dataset\n",
        "# Use fetch_20newsgroups with subset=\"train\" and remove=(\"headers\", \"footers\", \"quotes\")\n",
        "# Store the data in a variable called 'data' and documents in 'documents'\n",
        "\n",
        "data = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))  # YOUR CODE HERE\n",
        "documents = data.data  # YOUR CODE HERE\n",
        "\n",
        "print(\"Number of documents:\", len(documents))\n",
        "print(\"\\nExample document snippet:\\n\")\n",
        "print(documents[0][:600])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575a8efa",
      "metadata": {
        "id": "575a8efa"
      },
      "source": [
        "### Preprocessing matters\n",
        "\n",
        "Embeddings learn from the text you provide. If you remove tokens, they cannot contribute to meaning.\n",
        "\n",
        "We will do *light preprocessing*:\n",
        "- lowercase\n",
        "- tokenize\n",
        "- keep only alphabetic tokens\n",
        "- remove stopwords (common function words like *the*, *and*)\n",
        "\n",
        "This keeps the lab simple while preserving enough information for meaningful embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "764b3b23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "764b3b23",
        "outputId": "958c4c6d-45b0-4b3a-a247-693b92e9cd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in first document (first 50):\n",
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
          ]
        }
      ],
      "source": [
        "# Build stopword set once\n",
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess(text: str):\n",
        "    \"\"\"Convert raw text into a list of clean tokens.\n",
        "\n",
        "    Steps:\n",
        "    1) Lowercase\n",
        "    2) Tokenize\n",
        "    3) Keep alphabetic tokens only (drop numbers/punctuation)\n",
        "    4) Remove stopwords\n",
        "\n",
        "    Returns:\n",
        "        List[str]: cleaned tokens\n",
        "    \"\"\"\n",
        "    # TODO: Implement the preprocessing steps\n",
        "    # 1. Lowercase and tokenize using word_tokenize\n",
        "    # 2. Filter to keep only alphabetic tokens that are not stopwords\n",
        "\n",
        "    tokens = word_tokenize(text.lower())  # YOUR CODE HERE\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]  # YOUR CODE HERE (filter step)\n",
        "    #tokens = None  # YOUR CODE HERE (filter step)\n",
        "    return tokens\n",
        "\n",
        "# Tokenize the full corpus\n",
        "corpus = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"Tokens in first document (first 50):\")\n",
        "print(corpus[0][:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d62d47",
      "metadata": {
        "id": "70d62d47"
      },
      "source": [
        "---\n",
        "## Checkpoint 1 (Data + preprocessing)\n",
        "1. Why might we remove stopwords for this lab?\n",
        "2. Name one situation where removing stopwords could be a bad idea."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The presence of stopwords does not really add any semantic meaning to this corpus, as the objective of the lab is not detailed understanding of language rules and structure\n",
        "2. If the model is meant to teach how to write or if it's meant to write proper documents, the removal of stopwords would remove the model's ability to do that."
      ],
      "metadata": {
        "id": "vwJ0qN8ykTgp"
      },
      "id": "vwJ0qN8ykTgp"
    },
    {
      "cell_type": "markdown",
      "id": "ecbaba5a",
      "metadata": {
        "id": "ecbaba5a"
      },
      "source": [
        "---\n",
        "## Part 2 — Distributional semantics (concept)\n",
        "\n",
        "A central idea from modern NLP is:\n",
        "\n",
        "> **\"You shall know a word by the company it keeps.\"** (J. R. Firth)\n",
        "\n",
        "In practice:\n",
        "- a word's **context** = nearby words within a window\n",
        "- words that occur in similar contexts get similar vectors\n",
        "\n",
        "This is why embeddings can capture semantic similarity without hand-built resources like WordNet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43a14dd",
      "metadata": {
        "id": "c43a14dd"
      },
      "source": [
        "---\n",
        "## Part 3 — Word2Vec (Skip-gram)\n",
        "\n",
        "### What Word2Vec does (high level)\n",
        "Word2Vec learns word vectors by solving a **prediction task**.\n",
        "\n",
        "In **Skip-gram**, the model learns to:\n",
        "- take a **center word** and predict surrounding **context words**\n",
        "\n",
        "Key hyperparameters:\n",
        "- `vector_size`: embedding dimension (typical 50 to 300)\n",
        "- `window`: context window size\n",
        "- `min_count`: ignore rare words\n",
        "- `sg=1`: Skip-gram (sg=0 would be CBOW)\n",
        "\n",
        "> Note: In practice, Word2Vec is usually trained with **negative sampling** for speed, rather than a full softmax over the entire vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f626c1ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f626c1ca",
        "outputId": "da971f8c-7ad5-4a81-c158-dbd41422f72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18095\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train Word2Vec (Skip-gram)\n",
        "# Use: sentences=corpus, vector_size=100, window=5, min_count=5, workers=4, sg=1\n",
        "\n",
        "#w2v = None  # YOUR CODE HERE\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        ")\n",
        "\n",
        "print(\"Vocabulary size:\", len(w2v.wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de56c077",
      "metadata": {
        "id": "de56c077"
      },
      "source": [
        "### Inspect a learned vector\n",
        "\n",
        "The numbers themselves are not interpretable dimension-by-dimension.  \n",
        "Meaning comes from **relative position in the vector space**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c6e8e245",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6e8e245",
        "outputId": "a0af0e4e-987d-493f-f45f-8d0deae9c3dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.68695587,  0.8979324 ,  0.24567215, -0.76400185, -0.42890525,\n",
              "       -0.6721273 , -0.12757322,  1.2912223 , -1.298467  , -0.39563003],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# TODO: Inspect the first 10 values of the word vector for \"computer\"\n",
        "# Use w2v.wv[\"computer\"] to access the vector\n",
        "\n",
        "# YOUR CODE HERE\n",
        "w2v.wv[\"car\"][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468a6897",
      "metadata": {
        "id": "468a6897"
      },
      "source": [
        "### Word2Vec vs Bag-of-Words vs N-grams\n",
        "\n",
        "| Aspect | Bag-of-Words | N-grams | Word2Vec |\n",
        "|--------|--------------|---------|----------|\n",
        "| **Representation** | Sparse vector (word counts) | Sparse vector (sequence counts) | Dense vector (learned embeddings) |\n",
        "| **Dimensionality** | Vocabulary size (10K–100K+) | Exponentially larger (V^n) | Fixed, small (50–300) |\n",
        "| **Word order** | ❌ Ignored | ✅ Captures local sequences | ❌ Ignored (but context matters during training) |\n",
        "| **Semantic similarity** | ❌ No — \"cat\" and \"dog\" are orthogonal | ❌ No | ✅ Yes — similar words have similar vectors |\n",
        "| **Handles rare/unseen words** | ❌ OOV problem | ❌ OOV problem | ❌ OOV problem (FastText helps) |\n",
        "| **Sparsity** | Very high | Even higher | None (dense) |\n",
        "| **Training required** | No (just counting) | No (just counting) | Yes (neural network) |\n",
        "\n",
        "**Key takeaways:**\n",
        "- **Bag-of-Words** treats documents as unordered collections of words. Simple and fast, but loses word order and has no notion of similarity.\n",
        "- **N-grams** capture local word sequences (bigrams, trigrams), preserving some order. However, they explode in size and remain sparse.\n",
        "- **Word2Vec** learns dense, low-dimensional vectors where **semantically similar words are close together**. This enables similarity search, analogies, and better generalization to unseen data.\n",
        "\n",
        "> **Why this matters:** Bag-of-Words and N-grams suffer from the **curse of dimensionality** and cannot generalize across synonyms. Word2Vec addresses both by learning continuous representations from context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1059abf",
      "metadata": {
        "id": "d1059abf"
      },
      "source": [
        "---\n",
        "## Part 4 — Similarity search (nearest neighbors)\n",
        "\n",
        "A common industry use of embeddings is **semantic similarity**.\n",
        "\n",
        "Example:\n",
        "- If a user searches for *\"motel\"*, we may want to retrieve documents about *\"hotel\"*.\n",
        "- With one-hot vectors, these are unrelated (orthogonal).\n",
        "- With embeddings, similar words often end up near each other in space.\n",
        "\n",
        "We typically measure similarity using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d64442c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d64442c8",
        "outputId": "ee32e0c2-75fd-4684-d747-022e3882ae07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'car':\n",
            "  bike            0.927\n",
            "  big             0.891\n",
            "  moving          0.878\n",
            "  cars            0.872\n",
            "  dealer          0.865\n",
            "  bought          0.863\n",
            "  got             0.862\n",
            "  around          0.852\n",
            "  gets            0.849\n",
            "  getting         0.845\n",
            "  extra           0.836\n",
            "  little          0.829\n",
            "  buy             0.829\n",
            "  buying          0.825\n",
            "  oil             0.824\n",
            "  gpf             0.822\n",
            "  noticed         0.818\n",
            "  worked          0.817\n",
            "  paid            0.815\n",
            "  ferrari         0.813\n"
          ]
        }
      ],
      "source": [
        "# TODO: Find most similar words to \"computer\"\n",
        "# Use w2v.wv.most_similar(target, topn=10)\n",
        "\n",
        "target = \"car\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=20):\n",
        "  print(f\"  {word:15s} {score:.3f}\")\n",
        "\n",
        "#results = w2v.wv.most_similar(target, topn=10)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE - iterate through the results and print each word and score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41a3826-885b-45e8-ec08-a24534479275",
        "id": "eTfjcvi3oYJS"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'computer':\n",
            "  systems         0.922\n",
            "  network         0.915\n",
            "  digital         0.896\n",
            "  voice           0.894\n",
            "  products        0.889\n",
            "  commercial      0.880\n",
            "  bbs             0.865\n",
            "  software        0.863\n",
            "  design          0.863\n",
            "  technology      0.854\n",
            "  currently       0.850\n",
            "  development     0.844\n",
            "  computers       0.843\n",
            "  includes        0.843\n",
            "  remote          0.841\n",
            "  local           0.841\n",
            "  equipment       0.841\n",
            "  access          0.837\n",
            "  product         0.836\n",
            "  features        0.832\n"
          ]
        }
      ],
      "source": [
        "# TODO: Find most similar words to \"computer\"\n",
        "# Use w2v.wv.most_similar(target, topn=10)\n",
        "\n",
        "target = \"computer\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=20):\n",
        "  print(f\"  {word:15s} {score:.3f}\")\n",
        "\n",
        "#results = w2v.wv.most_similar(target, topn=10)\n",
        "\n",
        "\n",
        "# YOUR CODE HERE - iterate through the results and print each word and score"
      ],
      "id": "eTfjcvi3oYJS"
    },
    {
      "cell_type": "markdown",
      "id": "94743f2b",
      "metadata": {
        "id": "94743f2b"
      },
      "source": [
        "---\n",
        "## Checkpoint 2 (Interpretation)\n",
        "Look at the nearest neighbors you got for **computer**.\n",
        "1. Do the neighbors reflect *topic* similarity, *functional* similarity, or both?\n",
        "2. Pick 1 surprising neighbor and propose a reason it might appear."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The neighbors reflect both topic and functional similarity\n",
        "2. bbs - A surprising neighbor is 'bbs' (Bulletin Board System). This likely appears because the 20 Newsgroups dataset, from which the embeddings were learned, originates from an era (late 80s/early 90s) when BBSs were a prevalent form of online communication for computer enthusiasts. Thus, 'computer' and 'bbs' would have frequently co-occurred in discussions within these newsgroups, leading the model to place them semantically close"
      ],
      "metadata": {
        "id": "lw08Koa-mzR8"
      },
      "id": "lw08Koa-mzR8"
    },
    {
      "cell_type": "markdown",
      "id": "aa037657",
      "metadata": {
        "id": "aa037657"
      },
      "source": [
        "---\n",
        "## Part 5 — Vector algebra (analogies)\n",
        "\n",
        "### Why this is in the notebook\n",
        "Vector algebra (analogies) demonstrates one of the most remarkable properties of word embeddings: **semantic relationships are encoded as geometric directions**. This section is included because:\n",
        "\n",
        "1. **Validates embedding quality** — If embeddings capture meaningful structure, analogies should work\n",
        "2. **Illustrates the \"linear substructure\" hypothesis** — Relationships like gender, tense, or country-capital are often parallel vectors\n",
        "3. **Common interview/exam topic** — The king–queen analogy is a classic NLP question\n",
        "4. **Foundation for downstream tasks** — Understanding vector arithmetic helps with transfer learning and feature engineering\n",
        "\n",
        "### How vector algebra works\n",
        "\n",
        "Embeddings often capture relationships as **directions** in vector space. The idea is:\n",
        "\n",
        "> If the relationship between **king** and **man** is similar to the relationship between **queen** and **woman**, then their difference vectors should be approximately equal.\n",
        "\n",
        "Mathematically:\n",
        "$$\\vec{king} - \\vec{man} \\approx \\vec{queen} - \\vec{woman}$$\n",
        "\n",
        "Rearranging:\n",
        "$$\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$$\n",
        "\n",
        "### Classic example\n",
        "\n",
        "\\[ \\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen} \\]\n",
        "\n",
        "This works *sometimes* because the model learns consistent patterns across many contexts.\n",
        "\n",
        "### Other analogy types that embeddings can capture\n",
        "\n",
        "| Relationship | Example |\n",
        "|--------------|---------|\n",
        "| Verb tense | walk : walked :: swim : swam |\n",
        "| Country–capital | France : Paris :: Japan : Tokyo |\n",
        "| Comparative | big : bigger :: small : smaller |\n",
        "| Plural | cat : cats :: dog : dogs |\n",
        "\n",
        "### Important caveats\n",
        "\n",
        "- **Analogies are an intrinsic evaluation** — They test the embedding space itself, not real-world task performance\n",
        "- **Results are dataset-dependent** — A model trained on news articles may not capture analogies about cooking or medicine\n",
        "- **Not always reliable** — Even good embeddings can fail on specific analogies due to data biases or polysemy (words with multiple meanings)\n",
        "- **Sensitive to corpus size** — Small corpora may not provide enough context for consistent relationship patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "84f7ff75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84f7ff75",
        "outputId": "acb3fddf-323b-41c4-ef72-c800e8bc2c0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('anatolia', 0.9606443643569946),\n",
              " ('lake', 0.9601286053657532),\n",
              " ('savagely', 0.9585355520248413),\n",
              " ('percent', 0.9577797651290894),\n",
              " ('sea', 0.9565079212188721)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# TODO: Perform analogy using vector arithmetic\n",
        "# Use w2v.wv.most_similar(positive=[...], negative=[...], topn=5)\n",
        "# Try: king - man + woman = ?\n",
        "\n",
        "# YOUR CODE HERE\n",
        "w2v.wv.most_similar(\n",
        "    positive=[\"king\", \"woman\"],\n",
        "    negative=[\"man\"],\n",
        "    topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f3d29a",
      "metadata": {
        "id": "85f3d29a"
      },
      "source": [
        "### Why \"queen\" might not appear\n",
        "\n",
        "- **Training corpus matters** — Your model was trained on the 20 Newsgroups dataset, which is about tech, politics, religion, and sports. Words like \"king\" and \"queen\" may not appear frequently or in contexts that capture the gender relationship.\n",
        "\n",
        "- **Corpus size** — 20 Newsgroups has ~11,000 documents. The famous king-queen analogy was demonstrated on models trained on billions of words.\n",
        "\n",
        "- **Context diversity** — For the analogy to work, the model needs to see \"king\" and \"queen\" in parallel contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "17338756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17338756",
        "outputId": "8f1f757a-a002-4550-8282-8cb463d1fbde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Check if \"queen\" is in the vocabulary\n",
        "\"queen\" in w2v.wv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c5f97b61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5f97b61",
        "outputId": "883c745f-bb79-4f5b-f4c1-ba9e779a58ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity to 'queen': 0.7734\n"
          ]
        }
      ],
      "source": [
        "# TODO: Calculate the cosine similarity between the result vector and \"queen\"\n",
        "# 1. Compute result_vec = w2v.wv[\"king\"] - w2v.wv[\"man\"] + w2v.wv[\"woman\"]\n",
        "# 2. Get queen_vec = w2v.wv[\"queen\"]\n",
        "# 3. Calculate cosine similarity using: np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "\n",
        "#result_vec = None  # YOUR CODE HERE\n",
        "result_vec = w2v.wv[\"king\"] - w2v.wv[\"man\"] + w2v.wv[\"woman\"]\n",
        "queen_vec = w2v.wv[\"queen\"] # YOUR CODE HERE\n",
        "\n",
        "similarity = np.dot(result_vec, queen_vec) / (norm(result_vec) * norm(queen_vec)) # YOUR CODE HERE\n",
        "print(f\"Cosine similarity to 'queen': {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdc4b6e",
      "metadata": {
        "id": "6cdc4b6e"
      },
      "source": [
        "---\n",
        "## Checkpoint 3 (Why analogies are tricky)\n",
        "1. Why might analogy results be wrong even if the embeddings are \"good\"?\n",
        "2. Give one reason analogy evaluation might not correlate with task performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. If the corpus lacks sufficient examples or consistent contextual patterns for the specific relationship being tested (e.g., gender roles for 'king' and 'queen' in the 20 Newsgroups dataset), the analogy may fail. Furthermore, polysemy (words having multiple meanings) can dilute the intended relationship in the averaged word vector.\n",
        "2. The model may generate good analogies and relationships relative to the training corpus, but the learned relationships may be ineffective on external data could have a different type of relationship to the words that the model didn't learn from its corpus"
      ],
      "metadata": {
        "id": "54_3ozrXqtJm"
      },
      "id": "54_3ozrXqtJm"
    },
    {
      "cell_type": "markdown",
      "id": "dca84156",
      "metadata": {
        "id": "dca84156"
      },
      "source": [
        "---\n",
        "## Part 6 — Visualizing embeddings (debugging skill)\n",
        "\n",
        "In practice, visualization helps you:\n",
        "- sanity-check whether embeddings capture structure\n",
        "- detect domain problems (too small corpus, noisy preprocessing)\n",
        "- communicate results to non-technical stakeholders\n",
        "\n",
        "We will use **PCA** to reduce vectors to 2D.\n",
        "\n",
        "> Important: A 2D projection can distort distances. Treat it as a visual aid, not a perfect representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "898b3b04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "898b3b04",
        "outputId": "a86e3464-aaae-4440-e54c-599b98c89ab2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxwAAAIjCAYAAACJXB3EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdzBJREFUeJzt3Xl4DWf/x/HPSSL7IiIIQuxiiZ2iJEptraULqlqitrYURYv2aUVbpbuqVnV50KK7rdpaK0qqdmqJWEqpLagkEiSSM78//HIeRxISMk7C+3Vd56pzzz0z35lz0swnM3OPxTAMQwAAAABgAidHFwAAAADg9kXgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAYBMdHS2LxaLo6GhHl1JoREREqFatWrdkXRaLRVFRUdftFxUVJYvFYtcWEhKiyMhIcworRCIiIhQREeHoMvLs0KFDslgsmjlzpqNLueU6duyoAQMGOLoMO0uWLJG3t7dOnTrl6FKAQoHAAdxi3377rSwWi+bPn59lWp06dWSxWLRq1aos08qVK6dmzZrdihLtzJs3Tz169FDFihXl6empatWqaeTIkUpISLDrY7FY9Nlnn+W4nOXLl8tisWjKlCmm15wZnHJ6ff3116bXgNy7+vMqUqSIKlasqN69e+uvv/5ydHl58tFHHzk0FOzevVtRUVE6dOhQvi5348aNGjJkiGrWrCkvLy+VK1dO3bt31969e7P0jYiIsH2WTk5O8vX1VbVq1fT4449r+fLleVpvTEyMli1bptGjR9va8vp9SUpK0vjx41WnTh15e3vLw8NDtWrV0ujRo3Xs2LFs19u9e3dZLBa79V6pffv2qly5siZOnJin7QHuVC6OLgC409x9992SpLVr1+qBBx6wtSclJWnnzp1ycXFRTEyMWrVqZZt25MgRHTlyRI888sgtr3fgwIEqXbq0HnvsMZUrV047duzQ1KlT9fPPP2vLli3y8PDQfffdJz8/P82dO1f9+/fPdjlz586Vs7PzLd2GoUOHqlGjRlnamzZtestqKCji4uLk5FSw/8aU+XldunRJW7Zs0SeffKKffvpJO3bsUOnSpfNlHcuWLcuX5eTko48+UvHixR12Nmn37t0aP368IiIiFBISkm/LfeONNxQTE6Nu3bopLCxMJ06c0NSpU1W/fn398ccfWc7ylS1b1nYwnpKSov3792vevHmaPXu2unfvrtmzZ6tIkSLXXe9bb72l1q1bq3Llylmm5eb78tdff6lNmzY6fPiwunXrpoEDB8rV1VV//vmnPv/8c82fPz9LaEpKStKPP/6okJAQffXVV5o0aVKWM4aSNGjQII0aNUrjx4+Xj49PrvclcEcyANxyFSpUMBo3bmzXtmTJEsNisRg9e/Y02rVrZzdt7ty5hiRj4cKFN7Veq9VqnD9/Psfpq1atMiQZq1atsmu72qxZswxJxqeffmpr69evn+Hk5GQcPXo0S/8LFy4Yfn5+Rvv27W+q/tzK3I7vvvvO9HWFh4cbNWvWNH09hmEYkoxx48Zdt9+4ceOMwvS/95w+rylTphiSjNdffz3HeZOTk80uL09q1qxphIeH5/tyDx48aEgyZsyYcc1+3333XZaf4fwQExNjpKam2rXt3bvXcHNzM3r16mXXntPPRHp6uvH0008bkoznn3/+uus8efKk4eLiYnz22Wd27bn9vly6dMmoU6eO4enpaaxZsybL8hMTE40XXnghS/t///tfo0iRIsavv/5qSDKio6NzrM/Z2dn4/PPPr7stwJ2uYP+5C7hN3X333dq6dasuXLhga4uJiVHNmjXVoUMH/fHHH7JarXbTLBaLmjdvLklKT0/Xq6++qkqVKsnNzU0hISF64YUXlJqaareekJAQ3X///Vq6dKkaNmwoDw8PTZ8+XZL0zz//qGvXrvLy8lKJEiX07LPPZplfUrbXu2eemYmNjbW1PfbYY7JardlervTTTz8pMTFRvXr1srXNnj1bDRo0kIeHh4oVK6ZHHnlER44cyTLv+vXr1bFjR/n7+8vLy0thYWF6//33s92vN8JisWjIkCH67rvvVKNGDXl4eKhp06basWOHJGn69OmqXLmy3N3dFRERkeOlKps3b1azZs3k4eGhChUq6OOPP87SJzU1VePGjVPlypXl5uam4OBgPf/881n2e2pqqp599lkFBgbKx8dHnTt31j///JPteteuXatGjRrJ3d1dlSpVsn2+V7v6Ho6ZM2fKYrEoJiZGI0aMUGBgoLy8vPTAAw9kuS7darUqKipKpUuXlqenp1q1aqXdu3dnWealS5c0fvx4ValSRe7u7goICNDdd9+d58toMt1zzz2SpIMHD0r6370pu3fv1qOPPip/f3/bGcPc/kxkdw9Hbj8X6fL3tnHjxvL09JS/v79atmxpO2sSEhKiXbt2afXq1bbLfa5cV0JCgoYPH67g4GC5ubmpcuXKeuONN+x+1jP7RUZGys/PT0WLFlWfPn3sLmHMycyZM9WtWzdJUqtWrWw1XHlP1kcffaSaNWvKzc1NpUuX1uDBg3O17GbNmsnV1dWurUqVKqpZs6bd/weuxdnZWVOmTFGNGjU0depUJSYmXrP/Tz/9pPT0dLVp0yZXy7/6+/LDDz9o+/btevHFF23fkyv5+vpqwoQJWdrnzJmje++9V61atVJoaKjmzJmT7fpKlCihsLAwLVy4MFf1AXcyLqkCHODuu+/Wl19+qfXr19sOSGJiYtSsWTM1a9ZMiYmJ2rlzp8LCwmzTqlevroCAAElS//79NWvWLD388MMaOXKk1q9fr4kTJyo2NjbLvSFxcXHq2bOnBg0apAEDBqhatWq6cOGCWrdurcOHD2vo0KEqXbq0vvzyS/3666+5qv/EiROSpOLFi9vaWrZsqbJly2ru3LkaMWKEXf+5c+fK09NTXbt2lSRNmDBBL730krp3767+/fvr1KlT+uCDD9SyZUtt3bpVRYsWlXT5vo/7779fQUFBGjZsmEqVKqXY2FgtXrxYw4YNu26d586d0+nTp7O0BwQE2F0isWbNGi1atEiDBw+WJE2cOFH333+/nn/+eX300Ud6+umndfbsWb355pt64oknsuyns2fPqmPHjurevbt69uypb7/9Vk899ZRcXV31xBNPSLp80N65c2etXbtWAwcOVGhoqHbs2KH33ntPe/fu1YIFC2zL69+/v2bPnq1HH31UzZo106+//qr77rsvy3bs2LFDbdu2VWBgoKKiopSenq5x48apZMmS1903mZ555hn5+/tr3LhxOnTokCZPnqwhQ4bom2++sfUZO3as3nzzTXXq1Ent2rXT9u3b1a5dO128eNFuWVFRUZo4caL69++vxo0bKykpSZs2bdKWLVt077335rqmTAcOHJAk2/c+U7du3VSlShW9/vrrMgxDUt5+Jq6Ul89l/PjxioqKUrNmzfTKK6/I1dVV69ev16+//qq2bdtq8uTJeuaZZ+Tt7a0XX3xRkmyfxfnz5xUeHq6jR49q0KBBKleunH7//XeNHTtWx48f1+TJkyVJhmGoS5cuWrt2rZ588kmFhoZq/vz56tOnz3X3V8uWLTV06FBNmTJFL7zwgkJDQyXJ9t+oqCiNHz9ebdq00VNPPaW4uDhNmzZNGzduVExMTK4ucbqSYRg6efKkatasmet5nJ2d1bNnT7300ktau3Zttt/rTL///rsCAgJUvnz5XC376u/LokWLJEmPP/54rus7duyYVq1apVmzZkmSevbsqffee09Tp07NErgkqUGDBnbfEQA5cPAZFuCOtGvXLkOS8eqrrxqGcfnUv5eXlzFr1izDMAyjZMmSxocffmgYhmEkJSUZzs7OxoABAwzDMIxt27YZkoz+/fvbLXPUqFGGJOPXX3+1tZUvX96QZCxZssSu7+TJkw1JxrfffmtrS0lJMSpXrpyryzH69etnODs7G3v37rVrf+655wxJRlxcnK0tMTHRcHd3N3r27GkYhmEcOnTIcHZ2NiZMmGA3744dOwwXFxdbe3p6ulGhQgWjfPnyxtmzZ+36Wq3Wa9aXeclFTq/jx4/b+koy3NzcjIMHD9rapk+fbkgySpUqZSQlJdnax44da0iy6xseHm5IMt555x1bW2pqqlG3bl2jRIkSRlpammEYhvHll18aTk5OWS7t+Pjjjw1JRkxMjGEY//t8n376abt+jz76aJZLqrp27Wq4u7sbf//9t61t9+7dhrOzc5ZLqsqXL2/06dPH9n7GjBmGJKNNmzZ2+/PZZ581nJ2djYSEBMMwDOPEiROGi4uL0bVrV7vlRUVFGZLsllmnTh3jvvvuM/Iq8/P673//a5w6dco4duyY8dNPPxkhISGGxWIxNm7caBjG/y4Vy/wuZcrLz0R4eLjdJU+5/Vz27dtnODk5GQ888ICRkZFh1/fK/ZfTJVWvvvqq4eXlleVnZsyYMYazs7Nx+PBhwzAMY8GCBYYk480337T1SU9PN1q0aHFTl1TFx8cbrq6uRtu2be3qnzp1qm3f59WXX35pSMpySdH1LjOcP3++Icl4//33r7n8u+++22jQoEGW9tx+X+rVq2f4+fnlaZvefvttw8PDw/Zzv3fvXkOSMX/+/Gz7v/7664Yk4+TJk3laD3Cn4ZIqwAFCQ0MVEBCgtWvXSpK2b9+ulJQU2yhUzZo1U0xMjCRp3bp1ysjIsF0S8PPPP0tSlrMII0eOlHT5MoQrVahQQe3atbNr+/nnnxUUFKSHH37Y1ubp6amBAwdet/a5c+fq888/18iRI1WlShW7aY899pitT6YffvhBFy9etF1ONW/ePFmtVnXv3l2nT5+2vUqVKqUqVarYRujaunWrDh48qOHDh9vOeGTK7gbO7Lz88stavnx5llexYsXs+rVu3druBtsmTZpIkh566CG7m0Ez268eCcfFxUWDBg2yvXd1ddWgQYMUHx+vzZs3S5K+++47hYaGqnr16nbbnXkZSOZ2Z36+Q4cOtVvH8OHD7d5nZGRo6dKl6tq1q8qVK2drDw0NzfJ5X8vAgQPt9meLFi2UkZGhv//+W5K0cuVKpaen6+mnn7ab75lnnsmyrKJFi2rXrl3at29frtd/pSeeeEKBgYEqXbq07rvvPqWkpGjWrFlq2LChXb8nn3zS7n1efyaulNvPZcGCBbJarXr55Zez3Hyfm+/jd999pxYtWsjf399uPW3atFFGRoZ+++0327a4uLjoqaeess3r7Oyc7f7OixUrVigtLU3Dhw+3q3/AgAHy9fW95j7Kzp49ezR48GA1bdo0V2dfruTt7S3p8hnIazlz5oz8/f1znH6970tSUlKeb+aeM2eO7rvvPtt8VapUUYMGDXK8rCqzvuzOpAL4Hy6pAhzAYrGoWbNm+u2332S1WhUTE6MSJUrYRmJp1qyZpk6dKkm24JEZOP7++285OTllGbWlVKlSKlq0qO1AMVOFChWyrP/vv/9W5cqVsxwoVatW7Zp1r1mzRv369VO7du2yvfY5LCxMtWrV0ldffWV7XsTcuXNVvHhx20Hwvn37ZBhGlrCSKfOyjszLI27mGRe1a9fO1fXfVx6wS5Kfn58kKTg4ONv2s2fP2rWXLl1aXl5edm1Vq1aVdPn5CXfddZf27dun2NhYBQYGZltDfHy8pP99vpUqVbKbfvVnc+rUKV24cCHb/VitWjXbQfj1XL3tmQdQmduY+X26+vtWrFixLAeDr7zyirp06aKqVauqVq1aat++vR5//HHbpYHX8/LLL6tFixZydnZW8eLFFRoaKheXrL+mrv5O5/Vn4kq5/VwOHDggJycn1ahRI1fbkt16/vzzz1x9/kFBQbaD8kzX+9m8nsx9cPVyXF1dVbFixWvuo6udOHHCNjLd999/L2dn5zzVkpycLEm5CgPG/18yl53rfV98fX3zNKxybGystm7dqt69e2v//v229oiICH344YdKSkqSr69vtvXl9o8gwJ2KwAE4yN13360ff/xRO3bssN2/kalZs2Z67rnndPToUa1du1alS5dWxYoV7ebP7S84Dw+PfKl3+/bt6ty5s2rVqqXvv/8+2wNB6fJZjjFjxmjTpk0qW7asVq1apUGDBtn6W61WWSwW/fLLL9keqFx9oHUr5HTAlFP7tQ6CcmK1WlW7dm29++672U6/OtzcKvm5jS1bttSBAwe0cOFCLVu2TJ999pnee+89ffzxxzkOl3yl3AbEnL7TN3LQd6s+F6vVqnvvvVfPP/98ttMzA2pBl5iYqA4dOighIUFr1qy5oeGKd+7cKSlriL1aQEBAlnB/pet9X6pXr66tW7fqyJEjufocZ8+eLUl69tln9eyzz2aZ/sMPP6hv3752bZn1XXk/G4CsCByAg1z5PI6YmBi7S2YaNGggNzc3RUdH20ZpylS+fHlZrVbt27fPdjOoJJ08eVIJCQm5usGyfPny2rlzpwzDsDtIi4uLy7b/gQMH1L59e5UoUUI///zzNUNBz549NXbsWM2dO1fly5dXRkaG3ehUlSpVkmEYqlChwjUPsjL/wr9z585cj1LjKMeOHVNKSordWY7Msf0zL9WqVKmStm/frtatW1/zwDjz8z1w4IDdX6Ov/mwCAwPl4eGR7eVLOX2ONyLz+7R//367MwtnzpzJ9mCwWLFi6tu3r/r27avk5GS1bNlSUVFRuQocN1Pjjf5M5PZzqVSpkqxWq3bv3q26devm2C+nZVSqVEnJycnX/S6XL19eK1euVHJyst3PWW4/05zWn7kP4uLi7P54kZaWpoMHD+bqZ+zixYvq1KmT9u7dqxUrVtzQ2Z6MjAzbIBLZjRx1perVq+uHH37I8zoyderUSV999ZVmz56tsWPHXrOvYRiaO3euWrVqleXyQUl69dVXNWfOnCyB4+DBgypevHiOZ64AXMY9HICDNGzYUO7u7pozZ46OHj1qd4bDzc1N9evX14cffqiUlBS7X8yZ4SNzVJtMmX+hvdaoL1cu49ixY/r+++9tbefPn9cnn3ySpe+JEyfUtm1bOTk5aenSpdf9xVquXDm1aNFC33zzjWbPnq0KFSrYbduDDz4oZ2dnjR8/Pstf0Q3D0JkzZyRJ9evXV4UKFTR58uQsw3beyF/fzZSenm43HG1aWpqmT5+uwMBANWjQQNLlJxcfPXpUn376aZb5L1y4oJSUFElShw4dJCnLE9mv/rydnZ3Vrl07LViwQIcPH7a1x8bGaunSpfmyXdLl+1tcXFw0bdo0u/bMS/6ulPnZZfL29lblypWzHV42P93Mz0RuP5euXbvKyclJr7zySpZhbK/8Pnp5eWU7zGz37t21bt26bD+bhIQEpaen27YlPT3dbn9nZGTogw8+yHEbrpQZeq+uoU2bNnJ1ddWUKVPs6v3888+VmJh43f9vZGRkqEePHlq3bp2+++67G3p4ZkZGhoYOHarY2FgNHTo0y+VJV2vatKnOnj17w0+bf/jhh1W7dm1NmDBB69atyzL93LlzttHEYmJidOjQIfXt21cPP/xwllePHj20atWqLE8m37x58x35IFEgrzjDATiIq6urGjVqpDVr1sjNzc12YJqpWbNmeueddyTJLnDUqVNHffr00SeffKKEhASFh4drw4YNmjVrlrp27Wr3hPKcDBgwQFOnTlXv3r21efNmBQUF6csvv5Snp2eWvu3bt9dff/2l559/XmvXrrXd6C5dHvIzu+FOH3vsMQ0cOFDHjh2z/ULPVKlSJb322msaO3asDh06pK5du8rHx0cHDx7U/PnzNXDgQI0aNUpOTk6aNm2aOnXqpLp166pv374KCgrSnj17tGvXrlwdVK9ZsybL0K3S5XtNcntfQW6ULl1ab7zxhg4dOqSqVavqm2++0bZt2/TJJ5/Y7kl5/PHH9e233+rJJ5/UqlWr1Lx5c2VkZGjPnj369ttvbc9KqVu3rnr27KmPPvpIiYmJatasmVauXGl3TXmm8ePHa8mSJWrRooWefvpppaen64MPPlDNmjX1559/5su2lSxZUsOGDdM777yjzp07q3379tq+fbt++eUXFS9e3O4v6jVq1FBERIQaNGigYsWKadOmTfr+++81ZMiQfKklJzfzM5Hbz6Vy5cp68cUX9eqrr6pFixZ68MEH5ebmpo0bN6p06dK2p2o3aNBA06ZN02uvvabKlSurRIkSuueee/Tcc89p0aJFuv/++xUZGakGDRooJSVFO3bs0Pfff69Dhw6pePHi6tSpk5o3b64xY8bo0KFDqlGjhubNm3fdZ1Zkqlu3rpydnfXGG28oMTFRbm5uuueee1SiRAmNHTtW48ePV/v27dW5c2fFxcXpo48+UqNGjWwDPuRk5MiRWrRokTp16qR///3XdvlRpqvnT0xMtPU5f/687UnjBw4c0COPPKJXX331utty3333ycXFRStWrMjVgBZXK1KkiObNm6c2bdqoZcuW6t69u5o3b64iRYpo165dmjt3rvz9/TVhwgTNmTNHzs7OOQavzp0768UXX9TXX39tG5wgPj5ef/75p204bQDX4JCxsQAYhvG/YVabNWuWZdq8efMMSYaPj4+Rnp5uN+3SpUvG+PHjjQoVKhhFihQxgoODjbFjxxoXL16061e+fPkchyn9+++/jc6dOxuenp5G8eLFjWHDhhlLlizJMqSmrjG8bE5PVP73338NNzc3Q5Kxe/fubPv88MMPxt133214eXkZXl5eRvXq1Y3BgwfbDalrGIaxdu1a49577zV8fHwMLy8vIywszPjggw+yXWam6w2Le+XQspKMwYMH282f+VTnt956K9vlXvmE48whQDdt2mQ0bdrUcHd3N8qXL29MnTo1S11paWnGG2+8YdSsWdNwc3Mz/P39jQYNGhjjx483EhMTbf0uXLhgDB061AgICDC8vLyMTp06GUeOHMn2SeOrV682GjRoYLi6uhoVK1Y0Pv7442yfNJ7TsLiZQ4hevY1XfgfS09ONl156yShVqpTh4eFh3HPPPUZsbKwREBBgPPnkk7Z+r732mtG4cWOjaNGihoeHh1G9enVjwoQJtqGBc5LbJ8NnbtepU6eyTMvtz8TVw+IaRu4/F8O4/BTqevXq2fqFh4cby5cvt00/ceKEcd999xk+Pj5ZfkbOnTtnjB071qhcubLh6upqFC9e3GjWrJnx9ttv2+2jM2fOGI8//rjh6+tr+Pn5GY8//rixdevWXA2LaxiG8emnnxoVK1a0DY985Wc5depUo3r16kaRIkWMkiVLGk899VSWYaezkzn8c06va/X19vY2qlSpYjz22GPGsmXLrruuK3Xu3Nlo3bq1XVtuvy+Zzp49a7z88stG7dq1DU9PT8Pd3d2oVauWMXbsWOP48eNGWlqaERAQYLRo0eKay6lQoYJRr1492/tp06YZnp6edkNnA8iexTAK2LUJAIACLyEhQf7+/nrttdeynMUqyFq0aCE3NzetWLHC0aUgF9asWaOIiAjt2bMnx5HtHKVevXqKiIjQe++95+hSgAKPezgAANd04cKFLG2Z90tERETc2mJu0vHjxxlRqBBp0aKF2rZtqzfffNPRpdhZsmSJ9u3bd92b0QFcxj0cAIBr+uabbzRz5kx17NhR3t7eWrt2rb766iu1bdtWzZs3d3R5ufL777/b7iEYPXq0o8tBHvzyyy+OLiGL9u3b254nAuD6CBwAgGsKCwuTi4uL3nzzTSUlJdluJH/ttdccXVquffrpp/rll180fPjwLEObAgDMxT0cAAAAAEzDPRwAAAAATEPgAAAAAGCaO+oeDqvVqmPHjsnHx8fuYVUAAABAYWUYhs6dO6fSpUvLyangnU+4owLHsWPHFBwc7OgyAAAAgHx35MgRlS1b1tFlZHFHBQ4fHx9Jlz8MX19fB1cDAAAA3LykpCQFBwfbjnULmjsqcGReRuXr60vgAAAAwG2loN4yUPAu8gIAAABw2yBwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOBwkIiJCw4cPd3QZAAAAgKkIHAAAAABMQ+AAAAAAYBoCRwHw5ZdfqmHDhvLx8VGpUqX06KOPKj4+3ja9YcOGevvtt23vu3btqiJFiig5OVmS9M8//8hisWj//v23vHYAAADgWggcBcClS5f06quvavv27VqwYIEOHTqkyMhI2/Tw8HBFR0dLkgzD0Jo1a1S0aFGtXbtWkrR69WqVKVNGlStXdkD1AAAAQM5cHF0ApCeeeML274oVK2rKlClq1KiRkpOT5e3trYiICH3++efKyMjQzp075erqqh49eig6Olrt27dXdHS0wsPDHbgFAAAAQPY4w1EAbN68WZ06dVK5cuXk4+NjCw+HDx+WJLVo0ULnzp3T1q1btXr1aoWHhysiIsJ21mP16tWKiIhwUPUAAABAzggct0CG1dC6A2e0cNtRrTtwRhlWwzYtJSVF7dq1k6+vr+bMmaONGzdq/vz5kqS0tDRJUtGiRVWnTh1FR0fbwkXLli21detW7d27V/v27eMMBwAAAAokLqky2ZKdxzX+x906nnjR1hbk566UlMthYs+ePTpz5owmTZqk4OBgSdKmTZuyLCc8PFyrVq3Shg0bNGHCBBUrVkyhoaGaMGGCgoKCVLVq1VuzQQAAAEAecIbDREt2HtdTs7fYhQ1JOpF4UXtPntOhMykqV66cXF1d9cEHH+ivv/7SokWL9Oqrr2ZZVkREhJYuXSoXFxdVr17d1jZnzhzObgAAAKDAInCYJMNqaPyPu2VkMy2zbf1f/6pYQHHNnDlT3333nWrUqKFJkybZDYGbqUWLFrJarXbhIiIiQhkZGdy/AQAAgALLYhhGdsfEt6WkpCT5+fkpMTFRvr6+pq5r3YEz6vnpH9ft99WAu9S0UoCptQAAAOD2dSuPcW8EZzhMEn/u4vU75aEfAAAAUBgROExSwsc9X/sBAAAAhRGBwySNKxRTkJ+7LDlMt+jyaFWNKxS7lWUBAAAAtxSBwyTOThaN61RDkrKEjsz34zrVkLNTTpEEAAAAKPwIHCZqXytI0x6rr1J+9pdNlfJz17TH6qt9rSAHVQYAAADcGjz4z2TtawXp3hqltOHgv4o/d1ElfC5fRsWZDQAAANwJCBy3gLOThaFvAQAAcEfikioAAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0hSZwTJs2TWFhYfL19ZWvr6+aNm2qX375xdFlAQAAALiGQhM4ypYtq0mTJmnz5s3atGmT7rnnHnXp0kW7du1ydGkAAAAAcmAxDMNwdBE3qlixYnrrrbfUr1+/XPVPSkqSn5+fEhMT5evra3J1AAAAgPkK+jGui6MLuBEZGRn67rvvlJKSoqZNm+bYLzU1Vampqbb3SUlJt6I8AAAAAP+v0FxSJUk7duyQt7e33Nzc9OSTT2r+/PmqUaNGjv0nTpwoPz8/2ys4OPgWVgsAAACgUF1SlZaWpsOHDysxMVHff/+9PvvsM61evTrH0JHdGY7g4OACe7oJAAAAyKuCfklVoQocV2vTpo0qVaqk6dOn56p/Qf8wAAAAgLwq6Me4heqSqqtZrVa7MxgAAAAACpZCc9P42LFj1aFDB5UrV07nzp3T3LlzFR0draVLlzq6NAAAAAA5KDSBIz4+Xr1799bx48fl5+ensLAwLV26VPfee6+jSwMAAACQg0ITOD7//HNHlwAAAAAgjwr1PRwAAAAACjYCBwAAuOUiIiI0fPhwR5cB4BYgcAAAgFtu3rx5evXVV3PV99ChQ7JYLNq2bZu5ReXBzJkzVbRoUUeXARQKheYeDgAAcPsoVqyYQ9Z76dIlFSlSxCHrBu5UnOEAAAC33JWXVIWEhOj111/XE088IR8fH5UrV06ffPKJrW+FChUkSfXq1ZPFYlFERIRt2meffabQ0FC5u7urevXq+uijj2zTMs+MfPPNNwoPD5e7u7vmzJmjyMhIde3aVW+//baCgoIUEBCgwYMH69KlS7Z5U1NTNWrUKJUpU0ZeXl5q0qSJoqOjJUnR0dHq27evEhMTZbFYZLFYFBUVZdq+Ago7AgcAAHC4d955Rw0bNtTWrVv19NNP66mnnlJcXJwkacOGDZKkFStW6Pjx45o3b54kac6cOXr55Zc1YcIExcbG6vXXX9dLL72kWbNm2S17zJgxGjZsmGJjY9WuXTtJ0qpVq3TgwAGtWrVKs2bN0syZMzVz5kzbPEOGDNG6dev09ddf688//1S3bt3Uvn177du3T82aNdPkyZPl6+ur48eP6/jx4xo1atQt2EtA4cQlVQAAwOE6duyop59+WpI0evRovffee1q1apWqVaumwMBASVJAQIBKlSplm2fcuHF655139OCDD0q6fCZk9+7dmj59uvr06WPrN3z4cFufTP7+/po6daqcnZ1VvXp13XfffVq5cqUGDBigw4cPa8aMGTp8+LBKly4tSRo1apSWLFmiGTNm6PXXX5efn58sFotdPQCyR+AAAAAOFxYWZvt35oF8fHx8jv1TUlJ04MAB9evXTwMGDLC1p6eny8/Pz65vw4YNs8xfs2ZNOTs7294HBQVpx44dkqQdO3YoIyNDVatWtZsnNTVVAQEBedswAAQOAABgPiMjQ+c3bVb6qVNyCQyUDMNu+tU3clssFlmt1hyXl5ycLEn69NNP1aRJE7tpVwYJSfLy8soy/7XWl5ycLGdnZ23evDnLsry9vXOsCUD2CBwAAMBUScuW6eTrE5V+4oSt7cKJ40rL5UhVrq6ukqSMjAxbW8mSJVW6dGn99ddf6tWrV77WW69ePWVkZCg+Pl4tWrTIsaYr6wGQMwIHAAAwTdKyZTo6bHiWMxpG2iUlr16tpGXLrruMEiVKyMPDQ0uWLFHZsmXl7u4uPz8/jR8/XkOHDpWfn5/at2+v1NRUbdq0SWfPntWIESNuuOaqVauqV69e6t27t9555x3Vq1dPp06d0sqVKxUWFqb77rtPISEhSk5O1sqVK1WnTh15enrK09PzhtcJ3M4YpQoAAJjCyMjQydcnZgkb/z9Vki5Pvw4XFxdNmTJF06dPV+nSpdWlSxdJUv/+/fXZZ59pxowZql27tsLDwzVz5kzbMLo3Y8aMGerdu7dGjhypatWqqWvXrtq4caPKlSsnSWrWrJmefPJJ9ejRQ4GBgXrzzTdvep3A7cpiGNn+X+C2lJSUJD8/PyUmJsrX19fR5QAAcFtLWb9Bh68YLSon5WbNkleTxregIuD2VNCPcTnDAQAATJF+6lS+9gNQOBE4AACAKVz+//kZ+dUPQOFE4AAAAKbwbNhALqVKSRZL9h0sFrmUKiXPhg1ubWEAbikCBwAAMIXF2VklXxj7/2+uCh3//77kC2NluepZFwBuLwQOAABgGt+2bVXm/clyKVnSrt2lZEmVeX+yfNu2dVBlAG4VnsMBAABM5du2rXxat7Z70rhnwwac2QDuEAQOAABgOouzM0PfAncoLqkCAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAABAAbFnzx7dddddcnd3V926dR1dTr4gcAAAAAAFxLhx4+Tl5aW4uDitXLlSM2fOVNGiRR1d1k0hcAAAAAAFxIEDB3T33XerfPnyCggIcFgdaWlp+bYsAgcAAACQj77//nvVrl1bHh4eCggIUJs2bZSSkiKr1apXXnlFZcuWlZubm+rWraslS5bY5rNYLNq8ebNeeeUVWSwWRUREqG/fvkpMTJTFYpHFYlFUVJSmTp2qWrVq2eZbvHixJOnzzz+3tbVp00b/+c9/JF0OMV26dFHJkiXl7e2tRo0aacWKFXY1h4SE6NVXX1Xv3r3l6+urgQMHSpLWrl2rFi1ayMPDQ8HBwRo6dKhSUlLytD8IHAAAAEA+OX78uHr27KknnnhCsbGxio6O1oMPPijDMPT+++/rnXfe0dtvv60///xT7dq1U+fOnbVv3z7bvDVr1tTIkSN1/PhxLVq0SJMnT5avr6+OHz+u48ePa9SoUQoPD9fu3bt16tQpSVJMTIyky+FAki5duqR169YpIiJCkpScnKyOHTtq5cqV2rp1q9q3b69OnTrp8OHDdrW//fbbqlOnjrZu3aqXXnpJBw4cUPv27fXQQw/pzz//1DfffKO1a9dqyJAhedonFsMwjJvZqYVJUlKS/Pz8lJiYKF9fX0eXAwAAgNvMli1b1KBBAx06dEjly5e3m1amTBkNHjxYL7zwgq2tcePGatSokT788ENJUt26ddW1a1dFRUVJkmbOnKnhw4crISHBNo9hGAoMDNTHH3+shx9+WGFhYdqxY4dKliypEydOKCYmRq1atVJCQoI8PT2zrbNWrVp68sknbeEhJCRE9erV0/z58219+vfvL2dnZ02fPt3WtnbtWoWHhyslJUXu7u652iec4QAKmMjISHXt2tXRZQAAgNyyZkgH10g7vlcd3yS1bn2PateurW7duunTTz/V2bNnlZSUpGPHjql58+Z2szZv3lyxsbF5Wp3FYlHLli0VHR2thIQExcXFSZJSU1O1Z88erV69Wo0aNbKFjeTkZI0aNUqhoaEqWrSovL29FRsbm+UMR8OGDe3eb9++XTNnzpS3t7ft1a5dO1mtVh08eDDX9brkaeuAQi4tLU2urq6OLgMAANwudi+SloyWko5JkpwlLe8QpN8jX9Syfef1wQcf6MUXX9Ty5cvzdbURERH65JNPtGbNGoWFhWnTpk1q3ry5oqOjtXr1aoWHh9v6jho1SsuXL9fbb7+typUry8PDQw8//HCWG8O9vLzs3icnJ2vQoEEaOnRolvWXK1cu17VyhgOmOXfunHr16iUvLy8FBQXpvffeU0REhIYPHy5JOnv2rHr37i1/f395enqqQ4cOtmsYk5KS5OHhoV9++cVumfPnz5ePj4/Onz8vSTpy5Ii6d++uokWLqlixYurSpYsOHTpk6595tmDChAkqXbq0qlWrpkOHDslisWjevHlq1aqVPD09VadOHa1bt842X+YQdIsXL1a1atXk6emphx9+WOfPn9esWbMUEhIif39/DR06VBkZGbb5UlNTNWrUKJUpU0ZeXl5q0qSJoqOjsyx36dKlCg0Nlbe3t9q3b6/jx49LkqKiojRr1iwtXLjQdnPYlfMDAIACZPci6dvetrCRyXLuhJrvn6jxPRpo69atcnV11cqVK1W6dGnb/RaZYmJiVKNGjRxX4erqaneskSnzPo7vvvtOd999tyTp7rvv1ooVKxQTE2O7fyNzHZGRkXrggQdUu3ZtlSpVyu54KSf169fX7t27Vbly5SyvvPwBl8AB04wYMUIxMTFatGiRli9frjVr1mjLli226ZGRkdq0aZMWLVqkdevWyTAMdezYUZcuXZKvr6/uv/9+zZ07126Zc+bMUdeuXeXp6alLly6pXbt28vHx0Zo1axQTE2M7gL8ysa9cuVJxcXFavny5bRQHSXrxxRc1atQobdu2TVWrVlXPnj2Vnp5um37+/HlNmTJFX3/9tZYsWaLo6Gg98MAD+vnnn/Xzzz/ryy+/1PTp0/X999/b5hkyZIjWrVunr7/+Wn/++ae6deum9u3b24JU5nLffvttffnll/rtt990+PBhjRo1StLlv0B0797dFkKOHz+uZs2a5d+HAgAA8oc14/KZDdnfDr3+n3S9vuaiNh1L1+GvRmre99/r1KlTCg0N1XPPPac33nhD33zzjeLi4jRmzBht27ZNw4YNy3E1ISEhSk5O1sqVK3X69GnbH13DwsLk7++vuXPnqkWLFpIuB44FCxYoNTXV7tKtKlWqaN68edq2bZu2b9+uRx99VFar9bqbOHr0aP3+++8aMmSItm3bpn379mnhwoV5vmlcxh0kMTHRkGQkJiY6upTbXlJSklGkSBHju+++s7UlJCQYnp6exrBhw4y9e/cakoyYmBjb9NOnTxseHh7Gt99+axiGYcyfP9/w9vY2UlJSDMO4/Pm5u7sbv/zyi2EYhvHll18a1apVM6xWq20ZqamphoeHh7F06VLDMAyjT58+RsmSJY3U1FRbn4MHDxqSjM8++8zWtmvXLkOSERsbaxiGYcyYMcOQZOzfv9/WZ9CgQYanp6dx7tw5W1u7du2MQYMGGYZhGH///bfh7OxsHD161G5ftG7d2hg7dmyOy/3www+NkiVL2t736dPH6NKly/V3MgAAcJy/fjOMcb5ZXruf9jLaVXI2Aj0thpuzjKoVgo0PPvjAMAzDyMjIMKKioowyZcoYRYoUMerUqWM7rslUp04dY9y4cXZtTz75pBEQEGBIspvWpUsXw8XFxTh69KghyTh79qzh7+9v3HXXXXbzHzx40GjVqpXh4eFhBAcHG1OnTjXCw8ONYcOG2fqUL1/eeO+997Js5oYNG4x7773X8Pb2Nry8vIywsDBjwoQJedpV3MMBU/z111+6dOmSGjdubGvz8/NTtWrVJEmxsbFycXFRkyZNbNMDAgJUrVo1241THTt2VJEiRbRo0SI98sgj+uGHH+Tr66s2bdpIunwj0/79++Xj42O37osXL+rAgQO297Vr1872tF9YWJjt30FBQZKk+Ph4Va9eXZLk6empSpUq2fqULFlSISEh8vb2tmuLj4+XJO3YsUMZGRmqWrWq3XpSU1PtHtxz9XKDgoJsywAAAIVE8slsm0MDnbXksSvuhXjoXan2w5IkJycnjRs3TuPGjctxsdu2bcvSNm3aNE2bNi1L+4IFCyRdvhQ9c/n//vtvln4hISH69ddf7doGDx5s9z6nS6waNWqkZcuW5VhvbhSawDFx4kTNmzdPe/bskYeHh5o1a6Y33njDdgALxzOMDCUkbFRqarySkhJvenmurq56+OGHNXfuXD3yyCOaO3euevToIReXy1/b5ORkNWjQQHPmzMkyb2BgoO3fV98AlalIkSK2f1ssFkmyO7145fTMPtm1Zc6TnJwsZ2dnbd68Wc7Oznb9rgwp2S3DuHNGpwYA4PbgXTJ/+93GCs09HKtXr9bgwYP1xx9/aPny5bp06ZLatm2b5ycdwhzx8UsV83tLbdnaS7t2P6uExJfl4mLRihWf2vokJiZq7969kqTQ0FClp6dr/fr1tulnzpxRXFyc3Y1TvXr10pIlS7Rr1y79+uuv6tWrl21a/fr1tW/fPpUoUSLLjUx+fn63YKvt1atXTxkZGYqPj89ST6lSpXK9nJxuDgMAAAVI+WaSb2lJlhw6WCTfMpf73eEKTeBYsmSJIiMjVbNmTdWpU0czZ87U4cOHtXnzZkeXdseLj1+qHTsHKzX1hK3N09NJ97b11ov/maR5897Url271K9fPzk5OclisahKlSrq0qWLBgwYoLVr12r79u167LHHVKZMGXXp0sW2nJYtW6pUqVLq1auXKlSoYHcJVq9evVS8eHF16dJFa9as0cGDBxUdHa2hQ4fqn3/+uaX7QJKqVq2qXr16qXfv3po3b54OHjyoDRs2aOLEifrpp59yvZyQkBD9+eefiouL0+nTp3Xp0iUTqwYAADfEyVlq/8b/v7k6dPz/+/aTLve7wxWawHG1xMTLl+wUK1Ysxz6pqalKSkqyeyF/GUaG9u57RVeP0CBJTz0VoBo13NWr11i1adNGzZs3V2hoqO2plDNmzFCDBg10//33q2nTpjIMQz///HOWS5169uyp7du3253dkC7fC/Hbb7+pXLlyevDBBxUaGqp+/frp4sWLDnuS/IwZM9S7d2+NHDlS1apVU9euXbVx48Y8jVU9YMAAVatWTQ0bNlRgYGCW4fMAAEABUaOz1P0LyTfIvt239OX2Gp0dU1cBYzEK4cXjVqtVnTt3VkJCgtauXZtjv6ioKI0fPz5Le2JiosMOSG83Z8/+oS1be123X/16c+TqWltlypTRO++8o379+t2C6gAAAG4Ba4b09++XbyT3Lnn5MqpbeGYjKSlJfn5+BfYYt9DcNH6lwYMHa+fOndcMG5I0duxYjRgxwvY+KSlJwcHBZpd3R0lNzXl0pX37UnXkyCVVr+6mjPR1+uijSZJkd8kUAABAoefkLFVo4egqCqxCFziGDBmixYsX67ffflPZsmWv2dfNzU1ubm63qLI7k5tbiWtO/+7bBB05cknu7uPUsGETrVmzRsWLF79F1QEAAMDRCk3gMAxDzzzzjObPn6/o6GhVqFDB0SVBUtGijeTmVkqpqSd19X0cVaq4adrHwXJzK6XmzVbLYuGmKQAAgDtNoblpfPDgwZo9e7bmzp0rHx8fnThxQidOnNCFCxccXdodzWJxVtUqL2e+u3qqJKlqlZcIGwAAAHeoQnPTeOaD2a42Y8YMRUZG5moZBf2GmsIsPn6p9u57xW5oXDe3IFWt8pJKlGjnwMoAAABubwX9GLdQXVKFgqtEiXYKDGxje9K4m1sJFS3aiDMbAAAAd7hCEzhQ8FkszvL3v8vRZQAAAKAAKTT3cAAAAAAofAgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAB3uKioKNWtW9fRZeA2ReAAAABAvkhLS3N0CSiACBwAAAD5xGq16s0331TlypXl5uamcuXKacKECZKkHTt26J577pGHh4cCAgI0cOBAJScn2+aNjIxU165d9frrr6tkyZIqWrSoXnnlFaWnp+u5555TsWLFVLZsWc2YMcM2z6FDh2SxWPT111+rWbNmcnd3V61atbR69Wpbn5kzZ6po0aJ2dS5YsEAWi8U2ffz48dq+fbssFossFotmzpwpSUpISFD//v0VGBgoX19f3XPPPdq+fbttOZlnRj777DNVqFBB7u7u+b1LcRsgcAAAAOSTsWPHatKkSXrppZe0e/duzZ07VyVLllRKSoratWsnf39/bdy4Ud99951WrFihIUOG2M3/66+/6tixY/rtt9/07rvvaty4cbr//vvl7++v9evX68knn9SgQYP0zz//2M333HPPaeTIkdq6dauaNm2qTp066cyZM7mquUePHho5cqRq1qyp48eP6/jx4+rRo4ckqVu3boqPj9cvv/yizZs3q379+mrdurX+/fdf2/z79+/XDz/8oHnz5mnbtm03twNxezLuIImJiYYkIzEx0dGlAACA20xSUpLh5uZmfPrpp1mmffLJJ4a/v7+RnJxsa/vpp58MJycn48SJE4ZhGEafPn2M8uXLGxkZGbY+1apVM1q0aGF7n56ebnh5eRlfffWVYRiGcfDgQUOSMWnSJFufS5cuGWXLljXeeOMNwzAMY8aMGYafn59dPfPnzzeuPAwcN26cUadOHbs+a9asMXx9fY2LFy/atVeqVMmYPn26bb4iRYoY8fHx190/ME9BP8Z1cWzcAQAAKJwyrIY2HPxX8ecuqoSPu3Rqv1JTU9W6dessfWNjY1WnTh15eXnZ2po3by6r1aq4uDiVLFlSklSzZk05Of3vApSSJUuqVq1atvfOzs4KCAhQfHy83fKbNm1q+7eLi4saNmyo2NjYm9q+7du3Kzk5WQEBAXbtFy5c0IEDB2zvy5cvr8DAwJtaF25vBA4AAIA8WrLzuMb/uFvHEy/a2vwuHLvp5RYpUsTuvcViybbNarXmeplOTk4yDMOu7dKlS9edLzk5WUFBQYqOjs4y7cp7Qq4MUUB2uIcDAAAgD5bsPK6nZm+xCxuSlFCkuCwubnr/ix+yzBMaGqrt27crJSXF1hYTEyMnJydVq1btpmv6448/bP9OT0/X5s2bFRoaKkkKDAzUuXPn7NZ99b0Wrq6uysjIsGurX7++Tpw4IRcXF1WuXNnuVbx48ZuuGXcOAgcAAEAuZVgNjf9xt4xspllcXOXX5CFNfeMVzZw5SwcOHNAff/yhzz//XL169ZK7u7v69OmjnTt3atWqVXrmmWf0+OOP2y6nuhkffvih5s+frz179mjw4ME6e/asnnjiCUlSkyZN5OnpqRdeeEEHDhzQ3LlzbaNQZQoJCdHBgwe1bds2nT59WqmpqWrTpo2aNm2qrl27atmyZTp06JB+//13vfjii9q0adNN14w7B4EDAAAglzYc/DfLmY0r+TZ/RD4Nu2rsf15SaGioevToofj4eHl6emrp0qX6999/1ahRIz388MNq3bq1pk6dmi91TZo0SZMmTVKdOnW0du1aLVq0yHYWolixYpo9e7Z+/vln1a5dW1999ZWioqLs5n/ooYfUvn17tWrVSoGBgfrqq69ksVj0888/q2XLlurbt6+qVq2qRx55RH///Xe+hCTcOSzG1Rf13caSkpLk5+enxMRE+fr6OrocAABQyCzcdlTDvt523X7vP1JXXeqWMb2eQ4cOqUKFCtq6dStPCr+DFfRjXM5wAAAA5FIJn9w92C63/YA7AYEDAAAglxpXKKYgP3dZcphukRTk567GFYrdyrKAAq1QBY7ffvtNnTp1UunSpWWxWLRgwQJHlwQAAO4gzk4WjetUQ5KyhI7M9+M61ZCzU06RJH+FhITIMAwup0KBVqgCR0pKiurUqaMPP/zQ0aUAAIA7VPtaQZr2WH2V8rO/bKqUn7umPVZf7WsFOagyoGAqVA/+69Chgzp06ODoMgAAwB2ufa0g3VujlN2TxhtXKHbLzmwAhUmhChx5lZqaqtTUVNv7pKQkB1YDAABuJ85OFjWtFODoMoACr1BdUpVXEydOlJ+fn+0VHBzs6JIAAACAO8ptHTjGjh2rxMRE2+vIkSOOLgkAAAC4o9zWl1S5ubnJzc3N0WUAAAAAd6zb+gwHAAAAAMcqVGc4kpOTtX//ftv7gwcPatu2bSpWrJjKlSvnwMoAAAAAZKdQBY5NmzapVatWtvcjRoyQJPXp00czZ850UFUAAAAAclKoAkdERIQMw3B0GQAAAAByiXs4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAuE1FRERo+PDht2RdISEhmjx58i1ZFwoXAgcAAAAA0xA4AAAAkCsZGRmyWq2OLkOSdOnSJUeXgFwicAAAANzGrFarnn/+eRUrVkylSpVSVFSUbdq7776r2rVry8vLS8HBwXr66aeVnJxsmz5z5kwVLVpUixYtUo0aNeTm5qbDhw8rPj5enTp1koeHhypUqKA5c+bYrXPUqFG6//77be8nT54si8WiJUuW2NoqV66szz77TJK0ceNG3XvvvSpevLj8/PwUHh6uLVu22C3TYrFo2rRp6ty5s7y8vDRhwgRJ0sKFC1W/fn25u7urYsWKGj9+vNLT0/Nt/+HmETgAAABuY7NmzZKXl5fWr1+vN998U6+88oqWL18uSXJyctKUKVO0a9cuzZo1S7/++quef/55u/nPnz+vN954Q5999pl27dqlEiVKKDIyUkeOHNGqVav0/fff66OPPlJ8fLxtnvDwcK1du1YZGRmSpNWrV6t48eKKjo6WJB09elQHDhxQRESEJOncuXPq06eP1q5dqz/++ENVqlRRx44dde7cObtaoqKi9MADD2jHjh164okntGbNGvXu3VvDhg3T7t27NX36dM2cOdMWRlBAGHeQxMREQ5KRmJjo6FIAAABMFx4ebtx99912bY0aNTJGjx6dbf/vvvvOCAgIsL2fMWOGIcnYtm2brS0uLs6QZGzYsMHWFhsba0gy3nvvPcMwDOPs2bOGk5OTsXHjRsNqtRrFihUzJk6caDRp0sQwDMOYPXu2UaZMmRzrzsjIMHx8fIwff/zR1ibJGD58uF2/1q1bG6+//rpd25dffmkEBQXluOzbUUE/xuUMBwDof5cNZIqKilLdunUdVg8A3IgMa4Y2ntion//6WRtPbJQhQ2FhYXZ9goKCbGcjVqxYodatW6tMmTLy8fHR448/rjNnzuj8+fO2/q6urnbLiI2NlYuLixo0aGBrq169ut3/Q4sWLao6deooOjpaO3bskKurqwYOHKitW7cqOTlZq1evVnh4uK3/yZMnNWDAAFWpUkV+fn7y9fVVcnKyDh8+bFd7w4YN7d5v375dr7zyiry9vW2vAQMG6Pjx43bbAMdycXQBAFAQjRo1Ss8884yjywCAXFvx9wpN2jBJJ8+ftLX9c+ofBVwIsOtnsVhktVp16NAh3X///Xrqqac0YcIEFStWTGvXrlW/fv2UlpYmT09PSZKHh4csFkue64mIiFB0dLTc3NwUHh6uYsWKKTQ0VGvXrtXq1as1cuRIW98+ffrozJkzev/991W+fHm5ubmpadOmSktLs1uml5eX3fvk5GSNHz9eDz74YJb1u7u757lmmIMzHChwLBaLFixYkOv+0dHRslgsSkhIMK0m3Hm8vb0VEBBw/Y4AUACs+HuFRkSPsAsbknQp45J+++c3rfh7RZZ5Nm/eLKvVqnfeeUd33XWXqlatqmPHjl13XdWrV1d6ero2b95sa4uLi8vyezjzPo6VK1fa7tWIiIjQV199pb1799raJCkmJkZDhw5Vx44dVbNmTbm5uen06dPXraV+/fqKi4tT5cqVs7ycnDjMLSj4JFDgHD9+XB06dMjXZXJ5zO0vIiJCQ4YM0ZAhQ+Tn56fixYvrpZdekmEYkqSzZ8+qd+/e8vf3l6enpzp06KB9+/bluLzsvjP//e9/bb8Ig4KCNGTIEEnSE088YTcai3R5uMYSJUro888/z98NBYCrZFgzNGnDJBkyskzLbHtjwxvKsGbYTatcubIuXbqkDz74QH/99Ze+/PJLffzxx9ddX7Vq1dS+fXsNGjRI69ev1+bNm9W/f395eHjY9WvZsqXOnTunxYsX2wWOOXPmKCgoSFWrVrX1rVKlir788kvFxsZq/fr16tWrV5blZefll1/WF198ofHjx2vXrl2KjY3V119/rf/85z/XnRe3DoEDBUpaWppKlSolNzc3R5eCQmjWrFlycXHRhg0b9P777+vdd9+1DbkYGRmpTZs2adGiRVq3bp0Mw1DHjh1zPY77tGnTNHjwYA0cOFA7duzQokWLVLlyZUlS//79tWTJEh0/ftzWf/HixTp//rx69OiR/xsKAFfYEr8ly5mNq504f0Jb4u2Hma1Tp47effddvfHGG6pVq5bmzJmjiRMn5mqdM2bMUOnSpRUeHq4HH3xQAwcOVIkSJez6+Pv7q3bt2goMDFT16tUlXQ4hVqvV7v4NSfr888919uxZ1a9fX48//riGDh2aZXnZadeunRYvXqxly5apUaNGuuuuu/Tee++pfPnyudoO3CKOvmv9Virod/DficLDw43Bgwcbw4YNMwICAoyIiAhDkjF//nxbn5iYGKNOnTqGm5ub0aBBA2P+/PmGJGPr1q2GYRjGqlWrDEnGihUrjAYNGhgeHh5G06ZNjT179hiG8b8RNq58zZgxw7Barca4ceOM4OBgw9XV1QgKCjKeeeYZB+wF5Ifw8HAjNDTUsFqttrbRo0cboaGhxt69ew1JRkxMjG3a6dOnDQ8PD+Pbb781DOPy98TPz882fdy4cUadOnVs70uXLm28+OKLOa6/Ro0axhtvvGF736lTJyMyMjIftgwAru2nAz8ZtWbWuu7rpwM/ObpUmKSgH+NyhgMON2vWLLm6uiomJibLqdykpCR16tRJtWvX1pYtW/Tqq69q9OjR2S7nxRdf1DvvvKNNmzbJxcVFTzzxhCSpR48eGjlypGrWrKnjx4/r+PHj6tGjh3744Qe99957mj59uvbt26cFCxaodu3apm8v8keGYSjm7DnNP3lWMWfPyZB011132d3Y2LRpU+3bt0+7d++Wi4uLmjRpYpsWEBCgatWqKTY29rrrio+P17Fjx9S6desc+/Tv318zZsyQdHm0lV9++cX2HQQAMwV6BuZrPyC/MUoVHK5KlSp68803s502d+5cWSwWffrpp3J3d1eNGjV09OhRDRgwIEvfCRMm2E7RjhkzRvfdd58uXrwoDw8PeXt7y8XFRaVKlbL1P3z4sEqVKqU2bdqoSJEiKleunBo3bmzORiJf/XQqQf/Zd1THU/93OVRyUopcL6ZdY64bl5vriHv37q0xY8Zo3bp1+v3331WhQgW1aNHClHoA4Er1S9RXSc+Sij8fn+19HBZZVNKzpOqXqO+A6gDu4UABcOU43leLi4tTWFiY3dB2OYWCK8cIDwoKkiS7p55erVu3brpw4YIqVqyoAQMGaP78+UpPT89r+bjFfjqVoP47D9mFDUlKsxqK/n2dfjqVYGvLfFptjRo1lJ6ervXr19umnTlzRnFxcapRo8Z11+nj46OQkBCtXLkyxz4BAQHq2rWrZsyYoZkzZ6pv37553zgAuAHOTs4a03iMpMvh4kqZ70c3Hi1nJ+dbXhsgEThwq1kzpINrpB3fX/6vjCxjat+oIkWK2P6deVmN1WrNsX9wcLDi4uL00UcfycPDQ08//bRatmyZ65uIcetlGIb+s+9oNn+/+//p8SfU95lh2r1nj7766it98MEHGjZsmKpUqaIuXbpowIABWrt2rbZv367HHntMZcqUUZcuXXK17qioKL3zzjuaMmWK9u3bpy1btuiDDz6w69O/f3/NmjVLsbGx6tOnz01uLQDkXpvybfRuxLsq4Wl/o3VJz5J6N+JdtSnfxkGVAVxShVtp9yJpyWgp6Yoxvv+5JAX75ThLtWrVNHv2bKWmptpGrtq4cWOeV+3q6qqMjIws7R4eHurUqZM6deqkwYMHq3r16tqxY4fq1+e0c0H0R0JyljMbV3Jve7+Sz59X48ZN5OrirGHDhmngwIGSLo+oMmzYMN1///1KS0tTy5Yt9fPPP9sF1Wvp06ePLl68qPfee0+jRo1S8eLF9fDDD9v1adOmjYKCglSzZk2VLl36xjcUAG5Am/Jt1Cq4lbbEb9Gp86cU6Bmo+iXqc2YDDkfgwK2xe5H0bW/p6r9Np6dJ+5Zenl6jc5bZHn30Ub344osaOHCgxowZo8OHD+vtt9+WpDw99TQkJEQHDx7Utm3bVLZsWfn4+Oirr75SRkaGmjRpIk9PT82ePVseHh4MpVeAxadd+5I3i7OLfIY9p2mffqIHSvrbTfP399cXX3yR47yRkZGKjIy0vY+KilJUVJRdn0GDBmnQoEE5LiMlJUVnz55Vv379rlknAJjF2clZjUo1cnQZgB0uqYL5rBmXz2zkeCGMpCVjLve7iq+vr3788Udt27ZNdevW1YsvvqiXX35Zkuzu67iehx56SO3bt1erVq0UGBior776SkWLFtWnn36q5s2bKywsTCtWrNCPP/7I06ULsBKuufsbSW775Rer1ar4+Hi9+uqrKlq0qDp3zhqeAQC4U3GGA+b7+3f7y6iuEB35//dvJB293K9CC9uToTM1a9ZM27dvt72fM2eObVQp6fJTS6+ep27dunZtbm5u+v7777Osv2vXrjeyRXCQu4p6K8itiE6kXsoxvpZ2K6K7inrf0roOHz6sChUqqGzZspo5c6ZcXPhfKwAAmfitCPMlX/vpp9fr98UXX6hixYoqU6aMtm/frtGjR6t79+65GqoUtxdni0WvVSmj/jsPySL7c2YB711+ovirVcrIOQ+X2+WHkJCQLKEXAABcludLqo4fP67Zs2fr559/Vlqa/Zj3KSkpeuWVV/KtONwmvEveVL8TJ07oscceU2hoqJ599ll169ZNn3zyST4WiMLkvsCi+qxWiEq52d/sHeRWRJ/VCtF9gUUdUxgAAMiWxcjDn+U2btyotm3bymq16tKlSypTpowWLFigmjVrSrr8dN3SpUtnOxpQQZCUlCQ/Pz8lJibK19fX0eXcOawZ0uRaUtJxZX8fh0XyLS0N3yExkgZyKcMw9EdCsuLT0lXC1UV3FfW+5Wc2AAAoCAr6MW6eznC88MILeuCBB3T27FmdPHlS9957r8LDw7V161az6sPtwMlZav/G/7+5+oDw/9+3n0TYQJ44Wyxq7u+jB0r6q7m/D2EDAIACKk+BY/PmzRozZoycnJzk4+Ojjz76SKNGjVLr1q1v6NkIuIPU6Cx1/0LyDbJv9y19uT2bIXEBAABQ+OX5pvGLFy/avR8zZoxcXFzUtm1b/fe//823wnLy4Ycf6q233tKJEydUp04dffDBB2rcuLHp60U+qNFZqn7f5dGokk9evmejfDPObAAAANzG8hQ4atWqpd9//11hYWF27aNGjZLValXPnj3ztbirffPNNxoxYoQ+/vhjNWnSRJMnT1a7du0UFxenEiVKmLpu5BMnZ6lCC0dXAQAAgFskT5dU9e7dW2vXrs122vPPP6/x48fbno1ghnfffVcDBgxQ3759VaNGDX388cfy9PS8JWdWAAAAAORdnkapcqS0tDR5enrq+++/t3tYW58+fZSQkKCFCxdmmSc1NVWpqam290lJSQoODi6wd/ADAAAAeXVbjVJ18eJFLVq0SOfOncsyLSkpSYsWLbI7wM9Pp0+fVkZGhkqWtH9WQ8mSJXXixIls55k4caL8/Pxsr+DgYFNqAwAAAJC9PAWO6dOn6/3335ePj0+Wab6+vpoyZYo+/fTTfCvuZo0dO1aJiYm215EjRxxdEgAAAHBHyVPgmDNnjoYPH57j9OHDh+uLL7642ZqyVbx4cTk7O+vkyZN27SdPnlSpUqWyncfNzU2+vr52LwAAAAC3Tp4Cx759+1SnTp0cp4eFhWnfvn03XVR2XF1d1aBBA61cudLWZrVatXLlSjVt2tSUdQIAAAC4OXkKHOnp6Tp16lSO00+dOqX09PSbLionI0aM0KeffqpZs2YpNjZWTz31lFJSUtS3b1/T1gkAAADgxuXpORw1a9bUihUr1KBBg2ynL1u2TDVr1syXwrLTo0cPnTp1Si+//LJOnDihunXrasmSJVluJAcAAABQMOTpDMcTTzyhV199VYsXL84y7ccff9SECRP0xBNP5Ftx2RkyZIj+/vtvpaamav369WrSpImp6wMAAABw4/J0hmPgwIH67bff1LlzZ1WvXl3VqlWTJO3Zs0d79+5V9+7dNXDgQFMKBQAAAFD45OkMhyTNnj1b33zzjapWraq9e/cqLi5O1apV01dffaWvvvrKjBoBAAAAFFJ5OsORkZGht99+W4sWLVJaWpruv/9+RUVFycPDw6z6AAAAABRieTrD8frrr+uFF16Qt7e3ypQpoylTpmjw4MFm1QYAAACgkMtT4Pjiiy/00UcfaenSpVqwYIF+/PFHzZkzR1ar1az6AAAAABRieQochw8fVseOHW3v27RpI4vFomPHjuV7YQAAAAAKvzw/+M/d3d2urUiRIrp06VK+FgUAAADg9pCnm8YNw1BkZKTc3NxsbRcvXtSTTz4pLy8vW9u8efPyr0IAAAAAhVaeAkefPn2ytD322GP5VgwAAACA20ueAseMGTPMqgMAAADAbSjPD/4DAAAAgNwicAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgmkITOCZMmKBmzZrJ09NTRYsWdXQ5AAAAAHKh0ASOtLQ0devWTU899ZSjSwEAAACQSy6OLiC3xo8fL0maOXOmYwsBAAAAkGuFJnDciNTUVKWmptreJyUlObAaAAAA4M5TaC6puhETJ06Un5+f7RUcHOzokgAAAIA7ikMDx5gxY2SxWK752rNnzw0vf+zYsUpMTLS9jhw5ko/VAwAAALgeh15SNXLkSEVGRl6zT8WKFW94+W5ubnJzc7vh+QEAAADcHIcGjsDAQAUGBjqyBAAAAAAmKjQ3jR8+fFj//vuvDh8+rIyMDG3btk2SVLlyZXl7ezu2OAAAAADZKjSB4+WXX9asWbNs7+vVqydJWrVqlSIiIhxUFQAAAIBrsRiGYTi6iFslKSlJfn5+SkxMlK+vr6PLAQAAAG5aQT/Gva2HxQUAAADgWAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xSKwHHo0CH169dPFSpUkIeHhypVqqRx48YpLS3N0aUBAAAAuIZCETj27Nkjq9Wq6dOna9euXXrvvff08ccf64UXXnB0aQCA64iIiNDw4cNt70NCQjR58uRczTtz5kwVLVrUlLoAALeGi6MLyI327durffv2tvcVK1ZUXFycpk2bprffftuBlQEA8mrjxo3y8vLKVd8ePXqoY8eOJlcEADBToQgc2UlMTFSxYsWu2Sc1NVWpqam290lJSWaXBQC4jsDAwFz39fDwkIeHh4nVAADMViguqbra/v379cEHH2jQoEHX7Ddx4kT5+fnZXsHBwbeoQgAo+CIiIvTMM89o+PDh8vf3V8mSJfXpp58qJSVFffv2lY+PjypXrqxffvnFNs/OnTvVoUMHeXt7q2TJknr88cd1+vRp2/SUlBT17t1b3t7eCgoK0jvvvJNlvVdfUpWQkKBBgwapZMmScnd3V61atbR48WJJWS+pioqKUt26dfXll18qJCREfn5+euSRR3Tu3DlbH6vVqokTJ9ru+6tTp46+//77fNxzAIC8cGjgGDNmjCwWyzVfe/bssZvn6NGjat++vbp166YBAwZcc/ljx45VYmKi7XXkyBEzNwcACp1Zs2apePHi2rBhg5555hk99dRT6tatm5o1a6YtW7aobdu2evzxx3X+/HklJCTonnvuUb169bRp0yYtWbJEJ0+eVPfu3W3Le+6557R69WotXLhQy5YtU3R0tLZs2ZLj+q1Wqzp06KCYmBjNnj1bu3fv1qRJk+Ts7JzjPAcOHNCCBQu0ePFiLV68WKtXr9akSZNs0ydOnKgvvvhCH3/8sXbt2qVnn31Wjz32mFavXp0/Ow0AkCcWwzAMR6381KlTOnPmzDX7VKxYUa6urpKkY8eOKSIiQnfddZdmzpwpJ6e85aWkpCT5+fkpMTFRvr6+N1w3ANwOIiIilJGRoTVr1kiSMjIy5OfnpwcffFBffPGFJOnEiRMKCgrSunXrtGLFCq1Zs0ZLly61LeOff/5RcHCw4uLiVLp0aQUEBGj27Nnq1q2bJOnff/9V2bJlNXDgQNtZjZCQEA0fPlzDhw/XsmXL1KFDB8XGxqpq1apZapw5c6aGDx+uhIQESZfPcLz11ls6ceKEfHx8JEnPP/+8fvvtN/3xxx9KTU1VsWLFtGLFCjVt2tS2nP79++v8+fOaO3duvu9HAHC0gn6M69B7OAIDA3N9Le/Ro0fVqlUrNWjQQDNmzMhz2AAAZBUWFmb7t7OzswICAlS7dm1bW8mSJSVJ8fHx2r59u1atWiVvb+8syzlw4IAuXLigtLQ0NWnSxNZerFgxVatWLcf1b9u2TWXLls02bOQkJCTEFjYkKSgoSPHx8ZIuX3J7/vx53XvvvXbzpKWlqV69erleBwAg/xSKm8aPHj2qiIgIlS9fXm+//bZOnTplm1aqVCkHVgYAhYfVatXff/+t5ORkW2goUqSIXR+LxWLXZrFYbPMmJyerU6dOeuONN7IsOygoSPv3789zTTdyQ3h2NVutVklScnKyJOmnn35SmTJl7Pq5ubnleV0AgJtXKALH8uXLtX//fu3fv19ly5a1m+bAK8IAoNDYvXu3lixZYjdaX+blULlVv359/fDDDwoJCZGLS9ZfH5UqVVKRIkW0fv16lStXTpJ09uxZ7d27V+Hh4dkuMywsTP/884/27t2bp7McOalRo4bc3Nx0+PDhHNcJALi1CsV1SZGRkTIMI9sXAODadu/erW+//TbL0ODp6enat2+fdu/enavlDB48WP/++6969uypjRs36sCBA1q6dKn69u2rjIwMeXt7q1+/fnruuef066+/aufOnYqMjLzmJbDh4eFq2bKlHnroIS1fvlwHDx7UL7/8oiVLltzQtvr4+GjUqFF69tlnNWvWLB04cEBbtmzRBx98oFmzZt3QMgEAN6dQnOEAANwYq9V63YP3JUuWqHr16te9N6506dKKiYnR6NGj1bZtW6Wmpqp8+fJq3769bd633nrLdumVj4+PRo4cqcTExGsu94cfftCoUaPUs2dPpaSkqHLlynajTuXVq6++qsDAQE2cOFF//fWXihYtqvr16+uFF1644WUCAG6cQ0eputUK+h38AJDfDh48mKu/7Pfp00cVKlS4BRUBAPJbQT/GLRSXVAEAbkzmTdT51Q8AgLwicADAbSy7IWxvph8AAHlF4ACA21j58uWve3rd19dX5cuXv0UVAQDuNAQOALiNOTk5qX379tfsc+VN3wAA5Dd+wwDAba5GjRrq3r17ljMdvr6+6t69u2rUqOGgygAAdwKGxQWAO0CNGjVUvXp1uyeNly9fnjMbAADTETgA4A7h5OTE0LcAgFuOP20BAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAABRohmEoPT3d0WUAAG4QgQMAkK8iIiI0dOhQPf/88ypWrJhKlSqlqKgoSdKhQ4dksVi0bds2W/+EhARZLBZFR0dLkqKjo2WxWPTLL7+oQYMGcnNz09q1a7V9+3a1atVKPj4+8vX1VYMGDbRp0ybbctauXasWLVrIw8NDwcHBGjp0qFJSUm7hlgMAskPgAADku1mzZsnLy0vr16/Xm2++qVdeeUXLly/P0zLGjBmjSZMmKTY2VmFhYerVq5fKli2rjRs3avPmzRozZoyKFCkiSTpw4IDat2+vhx56SH/++ae++eYbrV27VkOGDDFj8wAAeeDi6AJyq3Pnztq2bZvi4+Pl7++vNm3a6I033lDp0qUdXRoA4CphYWEaN26cJKlKlSqaOnWqVq5cqSpVquR6Ga+88oruvfde2/vDhw/rueeeU/Xq1W3LzTRx4kT16tVLw4cPt02bMmWKwsPDNW3aNLm7u+fDVgEAbkShOcPRqlUrffvtt4qLi9MPP/ygAwcO6OGHH3Z0WQCAbISFhdm9DwoKUnx8fJ6W0bBhQ7v3I0aMUP/+/dWmTRtNmjRJBw4csE3bvn27Zs6cKW9vb9urXbt2slqtOnjw4I1vCADgphWaMxzPPvus7d/ly5fXmDFj1LVrV126dMl2Sh0AcOtZrRk6GrtLyQln5V3UX5KR5f/LFotFVqtVTk6X/85lGIZt2qVLl7JdrpeXl937qKgoPfroo/rpp5/0yy+/aNy4cfr666/1wAMPKDk5WYMGDdLQoUOzLKdcuXI3uYUAgJtRaALHlf7991/NmTNHzZo1u2bYSE1NVWpqqu19UlLSrSgPAO4Y+9b/rl9nfqLkf0/b2o7tjVNIqZLZ9g8MDJQkHT9+XPXq1ZMkuxvIr6dq1aqqWrWqnn32WfXs2VMzZszQAw88oPr162v37t2qXLnyjW8MAMAUheaSKkkaPXq0vLy8FBAQoMOHD2vhwoXX7D9x4kT5+fnZXsHBwbeoUgC4/e1b/7sWvfu6XdiQpIz0S/pry0btW/97lnk8PDx011132W4GX716tf7zn/9cd10XLlzQkCFDFB0drb///lsxMTHauHGjQkNDJV3+/fD7779ryJAh2rZtm/bt26eFCxdy0zgAFAAODRxjxoyRxWK55mvPnj22/s8995y2bt2qZcuWydnZWb1797Y7LX+1sWPHKjEx0fY6cuTIrdgsALjtWa0Z+nXmJ9fss2rWJ7JaM7K0//e//1V6eroaNGig4cOH67XXXrvu+pydnXXmzBn17t1bVatWVffu3dWhQweNHz9e0uV7RlavXq29e/eqRYsWqlevnl5++WUGFgGAAsBiXOuI3WSnTp3SmTNnrtmnYsWKcnV1zdL+zz//KDg4WL///ruaNm2aq/UlJSXJz89PiYmJ8vX1vaGaAeBmREREqG7dupo8ebJD1h8VFaUFCxbk6TKm7Dz79FP69qu5GtG2xTX7dX/5dQXXDLtmHwDAzSnox7gOvYcjMDDQdj1vXlmtVkmyu0cDAHBto0aN0jPPPGN7HxkZqYSEBC1YsCBPy0m7eCFX/ZITzuZpuQCA20+huGl8/fr12rhxo+6++275+/vrwIEDeumll1SpUqVcn90AAMg2ZOzNcnX3yN36ivrf9LoAAIVbobhp3NPTU/PmzVPr1q1VrVo19evXz3a9rpubm6PLA4A8sVqtev7551WsWDGVKlVKUVFRtmmHDx9Wly5d5O3tLV9fX3Xv3l0nT560Td++fbtatWolHx8f+fr6qkGDBtq0aZMkaebMmSpatKgWLFigKlWqyN3dXe3atbO7fy0qKkp169a1/XvWrFlauHCh7b656OhoSZdvwq5atao8PT1VsWJFvfTSS3bD1/oGBsrJ2fma2+kTUFxlQmve5N4CABR2heIMR+3atfXrr786ugwAyBezZs3SiBEjtH79eq1bt06RkZFq3ry5WrdubQsbq1evVnp6ugYPHqwePXrYgkCvXr1Ur149TZs2Tc7Oztq2bZvd8ODnz5/XhAkT9MUXX8jV1VVPP/20HnnkEcXExGSpY9SoUYqNjVVSUpJmzJghSSpWrJgkycfHRzNnzlTp0qW1Y8cODRgwQD4+Pnr++eclSRaLk7yLBVxzO1v1GSgnp2uHEgDA7a9QBA4AuJ2EhYVp3LhxkqQqVapo6tSpWrlypSRpx44dOnjwoG0Y7y+++EI1a9bUxo0b1ahRIx0+fFjPPfecqlevbpv/SpcuXdLUqVPVpEkTSZfDTWhoqDZs2KDGjRvb9fX29paHh4dSU1NVqlQpu2lXDlUbEhKiUaNG6euvv7YFDkly8/RS5xEvZHkOh09AcbXqM1BVmjS7qf0EALg9EDgA4BYLC7MftSkoKEjx8fGKjY1VcHCw3TODatSooaJFiyo2NlaNGjXSiBEj1L9/f3355Zdq06aNunXrpkqVKtn6u7i4qFGjRrb31atXt81/deC4lm+++UZTpkzRgQMHlJycrPT09GxHPqnSpJkqNWpi96TxMqE1ObMBALApFPdwAEBhZVgNXTyQoPPb4nXxQIIk2V0CJUkWi8U28t71REVFadeuXbrvvvv066+/qkaNGpo/f36+1rxu3Tr16tVLHTt21OLFi7V161a9+OKLSktLy7a/k5OzgmuGKbR5uIJrhhE2AAB2OMMBACa5sPO0En48oIzE/x2opx1OUnpw9kPKhoaG6siRIzpy5IjtLMfu3buVkJCgGjVq2PpVrVpVVatW1bPPPquePXtqxowZeuCBByRJ6enp2rRpk+1sRlxcnBISEmxP5L6aq6urMjLsH873+++/q3z58nrxxRdtbX///fcN7AEAADjDAQCmuLDztM7MjrULG5JkpBu6GPuvLuw8nWWeNm3aqHbt2urVq5e2bNmiDRs2qHfv3goPD1fDhg114cIFDRkyRNHR0fr7778VExOjjRs32oWJIkWK6JlnntH69eu1efNmRUZG6q677srxcqqQkBD9+eefiouL0+nTp3Xp0iVVqVJFhw8f1tdff60DBw5oypQp+X4WBQBw5yBwAEA+M6yGEn48cM0+CT/+JcNq2LVZLBYtXLhQ/v7+atmypdq0aaOKFSvqm2++kSQ5OzvrzJkz6t27t6pWraru3burQ4cOGj9+vG0Znp6eGj16tB599FE1b95c3t7etvmzM2DAAFWrVk0NGzZUYGCgYmJi1LlzZz377LMaMmSI6tatq99//10vvfTSTewRAMCdzGIYhnH9breHgv7YdwC3h4sHEnT60x3X7Vd8QG25Vyqab+udOXOmhg8froSEhHxbJgCg4Cvox7ic4QCAfGY9l/3N1TfaDwCAwozAAQD5zMnHNV/7AQBQmBE4ACCfuVXwk7PftcOEs5+b3Cr45et6IyMjuZwKAFDgEDgAIJ9ZnCwq2qnSNfsU7VRRFifLLaoIAADHIXAAgAk8ahVXwGOhWc50OPu5KeCxUHnUKu6gygAAuLV48B8AmMSjVnG51whQ6sFEWc+lycnHVW4V/DizAQC4oxA4AMBEFidLvg59CwBAYcMlVQAAAABMQ+AAAAAAYBoCBwAAAADTEDgAAAAAmIbAAQC3iYiICA0fPtzRZQAAYIfAAQDIgvACAMgvBA4AgGnS0tIcXQIAwMEIHABQCKWkpKh3797y9vZWUFCQ3nnnHbvpZ8+eVe/eveXv7y9PT0916NBB+/bts+sTExOjiIgIeXp6yt/fX+3atdPZs2cVGRmp1atX6/3335fFYpHFYtGhQ4ckSatXr1bjxo3l5uamoKAgjRkzRunp6bZlRkREaMiQIRo+fLiKFy+udu3amb4vAAAFG4EDAAqh5557TqtXr9bChQu1bNkyRUdHa8uWLbbpkZGR2rRpkxYtWqR169bJMAx17NhRly5dkiRt27ZNrVu3Vo0aNbRu3TqtXbtWnTp1UkZGht5//301bdpUAwYM0PHjx3X8+HEFBwfr6NGj6tixoxo1aqTt27dr2rRp+vzzz/Xaa6/Z1TZr1iy5uroqJiZGH3/88S3dLwCAgocnjQNAIZOcnKzPP/9cs2fPVuvWrSVdPsgvW7asJGnfvn1atGiRYmJi1KxZM0nSnDlzFBwcrAULFqhbt25688031bBhQ3300Ue25dasWdP2b1dXV3l6eqpUqVK2to8++kjBwcGaOnWqLBaLqlevrmPHjmn06NF6+eWX5eR0+W9YVapU0Ztvvmn6fgAAFA6c4QCAQubAgQNKS0tTkyZNbG3FihVTtWrVJEmxsbFycXGxmx4QEKBq1aopNjZW0v/OcORFbGysmjZtKovFYmtr3ry5kpOT9c8//9jaGjRocEPbBQC4PXGGAwAKAavV0PF9CUpJSlX8saSbXp6Hh0c+VJU9Ly8v05YNACh8OMMBAAXcga3x+uKF37Xgva1a/vlu7ViUJGcnFy38epmtz9mzZ7V3715JUmhoqNLT07V+/Xrb9DNnziguLk41atSQJIWFhWnlypU5rtPV1VUZGRl2baGhobb7QTLFxMTIx8fHdjkXAABXI3AAQAF2YGu8lkzfqZSEVFubWxEPNa3eQVGv/kezp/+gnTt3KjIy0u4eii5dumjAgAFau3attm/frscee0xlypRRly5dJEljx47Vxo0b9fTTT+vPP//Unj17NG3aNJ0+fVqSFBISovXr1+vQoUM6ffq0rFarnn76aR05ckTPPPOM9uzZo4ULF2rcuHEaMWKEbd0AAFyN3xAAUEBZrYbWfLMv22kP3DVIlYJqq98zj6lNmza6++677e6dmDFjhho0aKD7779fTZs2lWEY+vnnn1WkSBFJUtWqVbVs2TJt375djRs3VtOmTbVw4UK5uFy+0nbUqFFydnZWjRo1FBgYqMOHD6tMmTL6+eeftWHDBtWpU0dPPvmk+vXrp//85z/m7wwAQKFlMa48N36bS0pKkp+fnxITE+Xr6+vocgDgmo7GndWC97Zet1/XZ+upTDX/W1ARAKAgKujHuJzhAIACKiUp9fqd8tAPAABHIHAAQAHl5euWr/0AAHAEAgcAFFBBVYrKq+i1w4S3v5uCqhS9NQUBAHADCBwAUEA5OVnUokeVa/a5u3sVOTlZrtkHAABHInAAQAFWqV4JtR9UK8uZDm9/N7UfVEuV6pVwUGUAAOQOTxoHgAKuUr0SqlAn0PakcS/fy5dRcWYDAFAYEDgAoBBwcrIw9C0AoFDikioAAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAAAAwDQEDgAAAACmIXAAAAAAMA2BAwAAAIBpCBwAAAAATEPgAAAAAGAaF0cXcCsZhiFJSkpKcnAlAAAAQP7IPLbNPNYtaO6owHHu3DlJUnBwsIMrAQAAAPLXuXPn5Ofn5+gysrAYBTUKmcBqterYsWPy8fGRxWJxdDl5lpSUpODgYB05ckS+vr6OLueOwX53HPa9Y7DfHYd97xjsd8dh3+cPwzB07tw5lS5dWk5OBe+OiTvqDIeTk5PKli3r6DJumq+vLz+UDsB+dxz2vWOw3x2Hfe8Y7HfHYd/fvIJ4ZiNTwYtAAAAAAG4bBA4AAAAApiFwFCJubm4aN26c3NzcHF3KHYX97jjse8dgvzsO+94x2O+Ow76/M9xRN40DAAAAuLU4wwEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHIXYTz/9pCZNmsjDw0P+/v7q2rWro0u6o6Smpqpu3bqyWCzatm2bo8u5rR06dEj9+vVThQoV5OHhoUqVKmncuHFKS0tzdGm3pQ8//FAhISFyd3dXkyZNtGHDBkeXdFubOHGiGjVqJB8fH5UoUUJdu3ZVXFyco8u6I02aNEkWi0XDhw93dCm3vaNHj+qxxx5TQECAPDw8VLt2bW3atMnRZcEkBI5C6ocfftDjjz+uvn37avv27YqJidGjjz7q6LLuKM8//7xKly7t6DLuCHv27JHVatX06dO1a9cuvffee/r444/1wgsvOLq0284333yjESNGaNy4cdqyZYvq1Kmjdu3aKT4+3tGl3bZWr16twYMH648//tDy5ct16dIltW3bVikpKY4u7Y6yceNGTZ8+XWFhYY4u5bZ39uxZNW/eXEWKFNEvv/yi3bt365133pG/v7+jS4NJGBa3EEpPT1dISIjGjx+vfv36ObqcO9Ivv/yiESNG6IcfflDNmjW1detW1a1b19Fl3VHeeustTZs2TX/99ZejS7mtNGnSRI0aNdLUqVMlSVarVcHBwXrmmWc0ZswYB1d3Zzh16pRKlCih1atXq2XLlo4u546QnJys+vXr66OPPtJrr72munXravLkyY4u67Y1ZswYxcTEaM2aNY4uBbcIZzgKoS1btujo0aNycnJSvXr1FBQUpA4dOmjnzp2OLu2OcPLkSQ0YMEBffvmlPD09HV3OHSsxMVHFihVzdBm3lbS0NG3evFlt2rSxtTk5OalNmzZat26dAyu7syQmJkoS3+9baPDgwbrvvvvsvvswz6JFi9SwYUN169ZNJUqUUL169fTpp586uiyYiMBRCGX+RTcqKkr/+c9/tHjxYvn7+ysiIkL//vuvg6u7vRmGocjISD355JNq2LCho8u5Y+3fv18ffPCBBg0a5OhSbiunT59WRkaGSpYsaddesmRJnThxwkFV3VmsVquGDx+u5s2bq1atWo4u547w9ddfa8uWLZo4caKjS7lj/PXXX5o2bZqqVKmipUuX6qmnntLQoUM1a9YsR5cGkxA4CpAxY8bIYrFc85V5Lbskvfjii3rooYfUoEEDzZgxQxaLRd99952Dt6Jwyu2+/+CDD3Tu3DmNHTvW0SXfFnK736909OhRtW/fXt26ddOAAQMcVDlgjsGDB2vnzp36+uuvHV3KHeHIkSMaNmyY5syZI3d3d0eXc8ewWq2qX7++Xn/9ddWrV08DBw7UgAED9PHHHzu6NJjExdEF4H9GjhypyMjIa/apWLGijh8/LkmqUaOGrd3NzU0VK1bU4cOHzSzxtpXbff/rr79q3bp1cnNzs5vWsGFD9erVi7/O5FFu93umY8eOqVWrVmrWrJk++eQTk6u78xQvXlzOzs46efKkXfvJkydVqlQpB1V15xgyZIgWL16s3377TWXLlnV0OXeEzZs3Kz4+XvXr17e1ZWRk6LffftPUqVOVmpoqZ2dnB1Z4ewoKCrI7hpGk0NBQ/fDDDw6qCGYjcBQggYGBCgwMvG6/Bg0ayM3NTXFxcbr77rslSZcuXdKhQ4dUvnx5s8u8LeV230+ZMkWvvfaa7f2xY8fUrl07ffPNN2rSpImZJd6WcrvfpctnNlq1amU7o+fkxAna/Obq6qoGDRpo5cqVtmG2rVarVq5cqSFDhji2uNuYYRh65plnNH/+fEVHR6tChQqOLumO0bp1a+3YscOurW/fvqpevbpGjx5N2DBJ8+bNswz9vHfvXo5hbmMEjkLI19dXTz75pMaNG6fg4GCVL19eb731liSpW7duDq7u9lauXDm7997e3pKkSpUq8RdJEx09elQREREqX7683n77bZ06dco2jb+8568RI0aoT58+atiwoRo3bqzJkycrJSVFffv2dXRpt63Bgwdr7ty5WrhwoXx8fGz3y/j5+cnDw8PB1d3efHx8stwr4+XlpYCAAO6hMdGzzz6rZs2a6fXXX1f37t21YcMGffLJJ5y5vo0ROAqpt956Sy4uLnr88cd14cIFNWnSRL/++itjWOO2tHz5cu3fv1/79+/PEuwY2Tt/9ejRQ6dOndLLL7+sEydOqG7dulqyZEmWG8mRf6ZNmyZJioiIsGufMWPGdS85BAqjRo0aaf78+Ro7dqxeeeUVVahQQZMnT1avXr0cXRpMwnM4AAAAAJiGi6ABAAAAmIbAAQAAAMA0BA4AAAAApiFwAAAAADANgQMAAACAaQgcAAAAAExD4AAAAABgGgIHAAAAANMQOAAAAACYhsABAMiVyMhIWSwWWSwWubq6qnLlynrllVeUnp4uSTIMQ5988omaNGkib29vFS1aVA0bNtTkyZN1/vx5SdKuXbv00EMPKSQkRBaLRZMnT3bgFgEAbgUCBwAg19q3b6/jx49r3759GjlypKKiovTWW29Jkh5//HENHz5cXbp00apVq7Rt2za99NJLWrhwoZYtWyZJOn/+vCpWrKhJkyapVKlSjtwUAMAtYjEMw3B0EQCAgi8yMlIJCQlasGCBra1t27Y6d+6cnn32WfXo0UMLFixQly5d7OYzDENJSUny8/Ozaw8JCdHw4cM1fPjwW1A9AMBROMMBALhhHh4eSktL05w5c1StWrUsYUOSLBZLlrABALhzEDgAAHlmGIZWrFihpUuX6p577tG+fftUrVo1R5cFACiACBwAgFxbvHixvL295e7urg4dOqhHjx6KiooSV+cCAHLi4ugCAACFR6tWrTRt2jS5urqqdOnScnG5/GukatWq2rNnj4OrAwAURJzhAADkmpeXlypXrqxy5crZwoYkPfroo9q7d68WLlyYZR7DMJSYmHgrywQAFCAEDgDATevevbt69Oihnj176vXXX9emTZv0999/a/HixWrTpo1WrVolSUpLS9O2bdu0bds2paWl6ejRo9q2bZv279/v4C0AAJiFYXEBALmS3bC4V7Jarfrkk0/03//+V7t27ZKLi4uqVKmi3r17a8CAAfLw8NChQ4dUoUKFLPOGh4crOjra3A0AADgEgQMAAACAabikCgAAAIBpCBwAAAAATEPgAAAAAGAaAgcAAAAA0xA4AAAAAJiGwAEAAADANAQOAAAAAKYhcAAAAAAwDYEDAAAAgGkIHAAAAABMQ+AAAAAAYJr/A3Yc+fSqSy+/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# TODO: Visualize word embeddings using PCA\n",
        "# 1. Define a list of words to visualize\n",
        "# 2. Filter to keep only words in vocabulary\n",
        "# 3. Get vectors for those words\n",
        "# 4. Use PCA to reduce to 2D\n",
        "# 5. Plot with labels\n",
        "\n",
        "words = [\n",
        "    \"computer\", \"software\", \"hardware\", \"internet\",\n",
        "    \"doctor\", \"nurse\", \"hospital\", \"medicine\",\n",
        "    \"government\", \"policy\", \"law\", \"rights\"\n",
        "]\n",
        "\n",
        "# Keep only words that exist in the vocabulary\n",
        "words = [w for w in words if w in w2v.wv]\n",
        "\n",
        "# TODO: Get vectors and apply PCA\n",
        "vectors = np.array([w2v.wv[w] for w in words])  # YOUR CODE HERE\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "coords = pca.fit_transform(vectors)  # YOUR CODE HERE\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(9, 6))\n",
        "for i, w in enumerate(words):\n",
        "    plt.scatter(coords[i, 0], coords[i, 1])\n",
        "    plt.text(coords[i, 0] + 0.02, coords[i, 1] + 0.02, w)\n",
        "\n",
        "plt.title(\"Word2Vec Embeddings Projected to 2D (PCA)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb9d895",
      "metadata": {
        "id": "aeb9d895"
      },
      "source": [
        "---\n",
        "## Checkpoint 4 (Visualization reading)\n",
        "1. Which words are closest together in your plot?\n",
        "2. Do you see clusters (health vs tech vs politics)? If not, what might explain that?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. medicine/nurse/hospital/doctor. policy/government/rights/law. hardware/software/computer/internet.\n",
        "2. There are health, tech, politics clusters in the visualization"
      ],
      "metadata": {
        "id": "fUlLr-GiqDJ0"
      },
      "id": "fUlLr-GiqDJ0"
    },
    {
      "cell_type": "markdown",
      "id": "5b806f57",
      "metadata": {
        "id": "5b806f57"
      },
      "source": [
        "---\n",
        "## Part 7 — FastText (subword embeddings)\n",
        "\n",
        "### Why FastText exists\n",
        "Word2Vec learns a vector for each *word token*. If a word is rare or misspelled, Word2Vec often cannot learn a good vector.\n",
        "\n",
        "FastText improves this by representing a word as a bag of **character n-grams**.\n",
        "That means it can build vectors for:\n",
        "- rare words\n",
        "- morphological variants (plural, verb forms)\n",
        "- unseen words (depending on implementation)\n",
        "\n",
        "Industry relevance:\n",
        "- user-generated text (typos, slang)\n",
        "- domain text (technical words)\n",
        "- languages with rich morphology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d3aed49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3aed49d",
        "outputId": "085a76ee-9a01-4408-c657-b5875cec3926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText vocabulary size: 18095\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train FastText on the same corpus\n",
        "# Use: sentences=corpus, vector_size=100, window=5, min_count=5, workers=4\n",
        "\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(\"FastText vocabulary size:\", len(ft.wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4deba9",
      "metadata": {
        "id": "bb4deba9"
      },
      "source": [
        "### Compare Word2Vec vs FastText behavior\n",
        "\n",
        "We compare neighbors for a related word form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc0fbf67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc0fbf67",
        "outputId": "75475ecb-3f67-48d5-d284-25665bfabac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText neighbors for: computers\n",
            "  computes        0.975\n",
            "  comics          0.966\n",
            "  compute         0.961\n",
            "  compact         0.954\n",
            "  company         0.950\n",
            "  competitors     0.947\n",
            "  compete         0.945\n",
            "  compuadd        0.943\n",
            "  computer        0.942\n",
            "  items           0.940\n",
            "\n",
            "Word2Vec neighbors for: computers\n",
            "  products        0.971\n",
            "  crypt           0.965\n",
            "  additional      0.963\n",
            "  purchasing      0.961\n",
            "  capabilities    0.959\n",
            "  royalties       0.959\n",
            "  freeware        0.958\n",
            "  demo            0.956\n",
            "  shareware       0.956\n",
            "  patches         0.954\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compare similarity results for \"computers\" between FastText and Word2Vec\n",
        "\n",
        "query = \"computers\"\n",
        "\n",
        "# FastText neighbors\n",
        "print(\"FastText neighbors for:\", query)\n",
        "# YOUR CODE HERE\n",
        "for w, s in ft.wv.most_similar(query, topn=10):\n",
        "  print(f\"  {w:15s} {s:.3f}\")\n",
        "\n",
        "# Word2Vec neighbors (check if word exists first)\n",
        "if query in w2v.wv:\n",
        "    print(\"\\nWord2Vec neighbors for:\", query)\n",
        "    # YOUR CODE HERE\n",
        "    for w, s in w2v.wv.most_similar(query, topn=10):\n",
        "      print(f\"  {w:15s} {s:.3f}\")\n",
        "else:\n",
        "    print(\"\\nWord2Vec does not contain the token 'computers' in its vocabulary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0565d416",
      "metadata": {
        "id": "0565d416"
      },
      "source": [
        "### Summary: Word2Vec vs FastText\n",
        "\n",
        "| Aspect | Word2Vec | FastText |\n",
        "|--------|----------|----------|\n",
        "| **Unit of learning** | Whole words | Words + character n-grams (subwords) |\n",
        "| **Vocabulary** | Fixed at training time | Can generate vectors for unseen words |\n",
        "| **OOV (out-of-vocabulary) handling** | ❌ Cannot handle — returns error | ✅ Builds vector from subword pieces |\n",
        "| **Morphological variants** | Treated as separate tokens (run ≠ running) | Related by shared subwords (run ≈ running) |\n",
        "| **Typos and misspellings** | ❌ No vector available | ✅ Still produces reasonable vector |\n",
        "| **Training speed** | Faster | Slower (more parameters) |\n",
        "| **Model size** | Smaller | Larger (stores subword vectors) |\n",
        "| **Memory usage** | Lower | Higher |\n",
        "\n",
        "### When to use Word2Vec\n",
        "\n",
        "✅ **Choose Word2Vec when:**\n",
        "- Your vocabulary is **stable and well-defined** (e.g., curated product names)\n",
        "- You need **faster training** and smaller models\n",
        "- Your corpus is **clean** with minimal typos\n",
        "- You're working with **English or morphologically simple languages**\n",
        "- Memory and storage are constrained\n",
        "\n",
        "### When to use FastText\n",
        "\n",
        "✅ **Choose FastText when:**\n",
        "- You expect **out-of-vocabulary words** at inference time\n",
        "- Working with **user-generated content** (social media, reviews, chat)\n",
        "- Your domain has **technical jargon** or rare terminology\n",
        "- Working with **morphologically rich languages** (German, Finnish, Turkish, Arabic)\n",
        "- You need to handle **typos, slang, or informal spelling**\n",
        "- New words/products/entities appear frequently\n",
        "\n",
        "### Real-world example\n",
        "\n",
        "| Scenario | Best Choice | Reason |\n",
        "|----------|-------------|--------|\n",
        "| Search engine for product catalog | Word2Vec | Products are known; vocabulary is controlled |\n",
        "| Twitter sentiment analysis | FastText | Typos, slang, hashtags are common |\n",
        "| Medical NLP with rare drug names | FastText | Drug names share prefixes/suffixes |\n",
        "| News article classification | Word2Vec | Clean, edited text |\n",
        "| Customer support chatbot | FastText | Users make typos |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e3ebf4",
      "metadata": {
        "id": "95e3ebf4"
      },
      "source": [
        "---\n",
        "## Part 8 — Practical mini-task: Build a tiny \"semantic search\" demo\n",
        "\n",
        "In industry, a common workflow is:\n",
        "1) choose an embedding model  \n",
        "2) represent text units as vectors  \n",
        "3) retrieve nearest neighbors to answer queries\n",
        "\n",
        "We will implement a simplified version:\n",
        "- take a query word\n",
        "- find its nearest neighbors\n",
        "- interpret results\n",
        "\n",
        "This is not full document search yet, but it is the core idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "198ac0f2",
      "metadata": {
        "id": "198ac0f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be2fc5e2-9373-4cc8-8107-0830dfed0579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: motel\n",
            "  (word not in vocabulary)\n",
            "\n",
            "Query: hotel\n",
            "  interplanetary  0.990\n",
            "  alabama         0.989\n",
            "  melbourne       0.988\n",
            "  yale            0.988\n",
            "  paris           0.987\n",
            "  pa              0.987\n",
            "  atmospheric     0.987\n",
            "  barbara         0.986\n",
            "\n",
            "Query: space\n",
            "  nasa            0.906\n",
            "  shuttle         0.886\n",
            "  launch          0.878\n",
            "  station         0.829\n",
            "  research        0.811\n",
            "  aeronautics     0.803\n",
            "  development     0.795\n",
            "  commercial      0.784\n",
            "\n",
            "Query: religion\n",
            "  christian       0.981\n",
            "  religious       0.980\n",
            "  islam           0.977\n",
            "  christianity    0.973\n",
            "  teachings       0.972\n",
            "  christians      0.971\n",
            "  belief          0.970\n",
            "  religions       0.969\n",
            "\n",
            "Query: graphics\n",
            "  programming     0.973\n",
            "  unix            0.965\n",
            "  ibm             0.965\n",
            "  software        0.961\n",
            "  amiga           0.961\n",
            "  macintosh       0.960\n",
            "  interface       0.959\n",
            "  sgi             0.958\n"
          ]
        }
      ],
      "source": [
        "# TODO: Implement a semantic_neighbors function and test it with queries\n",
        "\n",
        "def semantic_neighbors(model, word: str, topn: int = 10):\n",
        "    \"\"\"Return nearest neighbors for a word, with a friendly error message.\"\"\"\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Check if word is in model.wv, if not return None\n",
        "    if word in model.wv:\n",
        "        return model.wv.most_similar(word, topn=topn)\n",
        "    # Otherwise return model.wv.most_similar(word, topn=topn)\n",
        "    else:\n",
        "        return None\n",
        "    pass\n",
        "\n",
        "queries = [\"motel\", \"hotel\", \"space\", \"religion\", \"graphics\"]\n",
        "\n",
        "for q in queries:\n",
        "    result = semantic_neighbors(w2v, q, topn=8)\n",
        "    print(\"\\nQuery:\", q)\n",
        "    if result is None:\n",
        "        print(\"  (word not in vocabulary)\")\n",
        "    else:\n",
        "        for w, s in result:\n",
        "            print(f\"  {w:15s} {s:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0d1f3c",
      "metadata": {
        "id": "8b0d1f3c"
      },
      "source": [
        "---\n",
        "## Checkpoint 5 (Industry thinking)\n",
        "Pick one query from the output above:\n",
        "1. Would these neighbors help a search engine user?\n",
        "2. What could go wrong if we deploy this directly in production?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the query: space\n",
        "\n",
        "1. The neighbors (nasa, launch, shuttle, station, research, etc.) would be very helpful for a search engine user specifically interested in outer space or space exploration. They accurately reflect related concepts within that domain, allowing the user to discover more relevant content or refine their search.\n",
        "\n",
        "2. A major issue would be the polysemy of the word \"space.\" While these results are great for outer space, a user searching for \"space\" might be looking for information on \"living space,\" \"disk space,\" \"personal space,\" or even \"white space\" in programming. Deploying this directly would lead to irrelevant results for these alternative meanings, causing user frustration due to the limited scope and bias of the training data towards a specific interpretation of \"space.\""
      ],
      "metadata": {
        "id": "6DHQXi1WyifK"
      },
      "id": "6DHQXi1WyifK"
    },
    {
      "cell_type": "markdown",
      "id": "4db1c0b0",
      "metadata": {
        "id": "4db1c0b0"
      },
      "source": [
        "---\n",
        "## Part 9 — Where does GloVe fit? (concept-only)\n",
        "\n",
        "You learned in lecture that we can also build a **co-occurrence matrix** \\(X\\) and try to compress it.\n",
        "\n",
        "Two major perspectives:\n",
        "1. **Predictive models** (Word2Vec): learn by predicting context  \n",
        "2. **Count-based global models** (GloVe): learn from aggregated co-occurrence statistics\n",
        "\n",
        "GloVe aims to encode meaning using **ratios of co-occurrence probabilities**, and often shows strong linear structure (useful for analogies).\n",
        "\n",
        "We do not train GloVe from scratch here because:\n",
        "- it requires building and storing large co-occurrence statistics\n",
        "- training is heavier than Word2Vec/FastText for an in-class lab\n",
        "\n",
        "However, you should understand the core difference:\n",
        "- Word2Vec: local prediction objective\n",
        "- GloVe: global co-occurrence objective\n",
        "\n",
        "**You will see GloVe again** when using pretrained embeddings in later weeks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54cb33a2",
      "metadata": {
        "id": "54cb33a2"
      },
      "source": [
        "---\n",
        "## Final Reflection (Submit)\n",
        "\n",
        "Write short answers (3 to 6 sentences each):\n",
        "\n",
        "1. How do embeddings solve the sparsity and \"no similarity\" issues of one-hot vectors?\n",
        "2. What is a realistic industry task where word embeddings are useful?\n",
        "3. When would you choose FastText over Word2Vec?\n",
        "4. What is one risk or limitation of word embeddings that a practitioner should watch for?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Embeddings create dense, low-dimensional vector representations for words, unlike the sparse, high-dimensional one-hot vectors. They capture semantic relationships by placing semantically similar words close together in the vector space, allowing for meaningful similarity comparisons that one-hot vectors cannot provide.\n",
        "\n",
        "2. A realistic industry task where word embeddings are highly useful is semantic search. Instead of matching exact keywords, embeddings allow search engines to understand the meaning behind a query and retrieve documents or products that are semantically related, even if they don't contain the exact query terms.\n",
        "\n",
        "3. You would choose FastText over Word2Vec when dealing with text that might contain many out-of-vocabulary (OOV) words, typos, or highly morphological languages. FastText's ability to represent words as character n-grams allows it to generate vectors for unseen words and better handle morphological variants or misspellings.\n",
        "\n",
        "4. One significant risk or limitation of word embeddings is that they can perpetuate biases present in the training data. If the corpus contains societal biases (e.g., gender stereotypes, racial biases), these can be encoded into the embeddings, leading to unfair or discriminatory outcomes in downstream applications like hiring tools or recommendation systems."
      ],
      "metadata": {
        "id": "0hLQaxdFxG71"
      },
      "id": "0hLQaxdFxG71"
    },
    {
      "cell_type": "markdown",
      "id": "eb6a47d1",
      "metadata": {
        "id": "eb6a47d1"
      },
      "source": [
        "---\n",
        "## Assignment\n",
        "\n",
        "- Change `window` (2, 10) and compare neighbors for the same query word.\n",
        "- Increase `vector_size` (50 vs 200) and compare results.\n",
        "- Try CBOW: set `sg=0` and compare results with Skip-gram.\n",
        "- Add bigrams using `gensim.models.Phrases` before training and observe changes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change window (2, 10) for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")\n",
        "print(\"Word2Vec vocabulary size:\", len(w2v.wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geZUbNxqy6y5",
        "outputId": "6496f0fa-af71-48da-fd95-e45d2c86e5cc"
      },
      "id": "geZUbNxqy6y5",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec vocabulary size: 18095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHqyuZR22Moc",
        "outputId": "54b5a4b5-3290-4462-a1bf-8f5e45b58fdf"
      },
      "id": "lHqyuZR22Moc",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  shuttle         0.839\n",
            "  research        0.824\n",
            "  nasa            0.815\n",
            "  station         0.810\n",
            "  launch          0.804\n",
            "  center          0.799\n",
            "  aeronautics     0.769\n",
            "  national        0.768\n",
            "  development     0.758\n",
            "  local           0.751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=2, vector_size=50 for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=50,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "7iNLr71hABEV"
      },
      "id": "7iNLr71hABEV",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QLtnErynPrw",
        "outputId": "4c6a89be-9acd-4e2e-bd66-4f4e1a7419b4"
      },
      "id": "5QLtnErynPrw",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  shuttle         0.837\n",
            "  station         0.810\n",
            "  launch          0.808\n",
            "  research        0.802\n",
            "  nasa            0.799\n",
            "  technology      0.776\n",
            "  aeronautics     0.762\n",
            "  development     0.757\n",
            "  center          0.754\n",
            "  flight          0.746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=2, vector_size=200 for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=200,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "KmVnJ3wQnsKJ"
      },
      "id": "KmVnJ3wQnsKJ",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZEUg9GCnwc2",
        "outputId": "42770515-a6b2-454c-8f91-2ddda7634fe5"
      },
      "id": "IZEUg9GCnwc2",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  nasa            0.853\n",
            "  research        0.853\n",
            "  shuttle         0.846\n",
            "  station         0.843\n",
            "  center          0.824\n",
            "  local           0.823\n",
            "  launch          0.803\n",
            "  computer        0.792\n",
            "  institute       0.784\n",
            "  engineering     0.783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change window to 10 for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "lDUYklO60dCW"
      },
      "id": "lDUYklO60dCW",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDHho20AnErH",
        "outputId": "b2e1674a-7420-4e7c-c196-61f097f945d3"
      },
      "id": "CDHho20AnErH",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  shuttle         0.892\n",
            "  nasa            0.891\n",
            "  launch          0.884\n",
            "  commercial      0.829\n",
            "  project         0.815\n",
            "  satellite       0.810\n",
            "  lunar           0.801\n",
            "  station         0.797\n",
            "  navigation      0.791\n",
            "  missions        0.791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=10, vector_size=50 for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=50,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "2EikUmBTotSo"
      },
      "id": "2EikUmBTotSo",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4ybGUFo1MD",
        "outputId": "4495a6d9-ce00-4c18-90f7-7c59961cbf6c"
      },
      "id": "aW4ybGUFo1MD",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  nasa            0.893\n",
            "  shuttle         0.885\n",
            "  launch          0.879\n",
            "  project         0.847\n",
            "  lunar           0.819\n",
            "  commercial      0.809\n",
            "  missions        0.801\n",
            "  callers         0.791\n",
            "  station         0.791\n",
            "  satellite       0.787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=10, vector_size=200 for Word2Vec and compare neighbors for the same query word.\n",
        "w2v = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=200,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        "  )"
      ],
      "metadata": {
        "id": "fE5fnijoo6zu"
      },
      "id": "fE5fnijoo6zu",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in w2v.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC2pJxgYpDHd",
        "outputId": "49e24651-472f-4118-c9b9-94ddad3d0461"
      },
      "id": "dC2pJxgYpDHd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  shuttle         0.905\n",
            "  nasa            0.904\n",
            "  launch          0.886\n",
            "  station         0.830\n",
            "  satellite       0.813\n",
            "  commercial      0.810\n",
            "  lunar           0.808\n",
            "  development     0.804\n",
            "  project         0.804\n",
            "  missions        0.799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FastText"
      ],
      "metadata": {
        "id": "Envc3Wpg9paO"
      },
      "id": "Envc3Wpg9paO"
    },
    {
      "cell_type": "code",
      "source": [
        "# window=2, vector_size=50 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=50,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "1EbZ-Xf9qGs_"
      },
      "id": "1EbZ-Xf9qGs_",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwgHdqiDqm4U",
        "outputId": "77c0c03e-370c-4095-a52b-ae502af39c88"
      },
      "id": "kwgHdqiDqm4U",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.977\n",
            "  spacepac        0.975\n",
            "  airspace        0.965\n",
            "  pace            0.954\n",
            "  aerospace       0.953\n",
            "  spacewalk       0.936\n",
            "  spacelink       0.921\n",
            "  spacecraft      0.913\n",
            "  spaces          0.912\n",
            "  launch          0.910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=2, vector_size=200 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=200,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "_OQKMKubqF-L"
      },
      "id": "_OQKMKubqF-L",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2U20B93qjXU",
        "outputId": "8a3dda70-8153-424a-a7f5-82745447a25b"
      },
      "id": "X2U20B93qjXU",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.980\n",
            "  spacepac        0.978\n",
            "  airspace        0.962\n",
            "  pace            0.949\n",
            "  aerospace       0.944\n",
            "  spacewalk       0.926\n",
            "  spacelink       0.923\n",
            "  spaces          0.914\n",
            "  spacecraft      0.912\n",
            "  computational   0.905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=10, vector_size=50 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=50,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "nqVhJiCYqFwE"
      },
      "id": "nqVhJiCYqFwE",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4uOw5osqFl5",
        "outputId": "08ab976e-b6d4-4f6d-fddb-8fef440e477b"
      },
      "id": "C4uOw5osqFl5",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.990\n",
            "  pace            0.985\n",
            "  spacepac        0.984\n",
            "  airspace        0.980\n",
            "  spacewalk       0.976\n",
            "  aerospace       0.970\n",
            "  spacewalks      0.969\n",
            "  spacelink       0.966\n",
            "  spacecraft      0.955\n",
            "  satellite       0.946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=10, vector_size=200 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=200,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "NtozGnGQqsTf"
      },
      "id": "NtozGnGQqsTf",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgCv_cdCqsKz",
        "outputId": "e331e2ff-bb8f-437d-ee54-4e3f2c354997"
      },
      "id": "bgCv_cdCqsKz",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.990\n",
            "  pace            0.985\n",
            "  spacepac        0.985\n",
            "  airspace        0.981\n",
            "  spacewalk       0.977\n",
            "  aerospace       0.970\n",
            "  spacewalks      0.968\n",
            "  spacelink       0.965\n",
            "  spaces          0.958\n",
            "  spacecraft      0.955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=2, vector_size=100 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=2,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "1ZsKkMjQqvIc"
      },
      "id": "1ZsKkMjQqvIc",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agYvf0lU-IeN",
        "outputId": "73ca77cb-d0c1-41ea-893a-e87c3e23e8cb"
      },
      "id": "agYvf0lU-IeN",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.976\n",
            "  spacepac        0.975\n",
            "  airspace        0.960\n",
            "  pace            0.953\n",
            "  aerospace       0.949\n",
            "  spacewalk       0.925\n",
            "  spacecraft      0.918\n",
            "  spacelink       0.916\n",
            "  spaces          0.907\n",
            "  cyberspace      0.901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# window=10, vector_size=100 for FastText\n",
        "ft = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=10,\n",
        "    min_count=5,\n",
        "    workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "_534GtI0-ILf"
      },
      "id": "_534GtI0-ILf",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"space\"\n",
        "print(f\"Most similar words to '{target}':\")\n",
        "\n",
        "for word, score in ft.wv.most_similar(target, topn=10):\n",
        "  print(f\"  {word:15s} {score:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0TQiKAiqu9M",
        "outputId": "530008f1-57ba-4eeb-a5fd-301b9c243bf2"
      },
      "id": "L0TQiKAiqu9M",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar words to 'space':\n",
            "  spacehab        0.990\n",
            "  pace            0.988\n",
            "  spacepac        0.986\n",
            "  airspace        0.979\n",
            "  spacewalk       0.977\n",
            "  spacelink       0.968\n",
            "  aerospace       0.967\n",
            "  spacewalks      0.967\n",
            "  spacecraft      0.956\n",
            "  satellite       0.954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try CBOW: set sg=0 and compare results with Skip-gram.\n"
      ],
      "metadata": {
        "id": "2-vagNQK-53o"
      },
      "id": "2-vagNQK-53o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW Architecture"
      ],
      "metadata": {
        "id": "tRixLPyyuPk4"
      },
      "id": "tRixLPyyuPk4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90cc8be3",
        "outputId": "63d8e65d-35a0-4a3d-a28e-9a9e31ff6333"
      },
      "source": [
        "w2v_cbow = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    sg=0 # CBOW architecture\n",
        ")\n",
        "\n",
        "print(\"CBOW Vocabulary size:\", len(w2v_cbow.wv))"
      ],
      "id": "90cc8be3",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Vocabulary size: 18095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42acfc7b",
        "outputId": "f654758e-70af-4827-a6be-e0a17bad6b86"
      },
      "source": [
        "print(\"Top 10 most similar words to 'space' (CBOW):\")\n",
        "for word, score in w2v_cbow.wv.most_similar('space', topn=10):\n",
        "    print(f\"  {word:15s} {score:.3f}\")"
      ],
      "id": "42acfc7b",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to 'space' (CBOW):\n",
            "  nasa            0.886\n",
            "  shuttle         0.880\n",
            "  launch          0.860\n",
            "  aeronautics     0.801\n",
            "  research        0.799\n",
            "  station         0.794\n",
            "  commercial      0.782\n",
            "  center          0.778\n",
            "  satellite       0.771\n",
            "  development     0.770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-gram"
      ],
      "metadata": {
        "id": "Nv0knLBSuo5q"
      },
      "id": "Nv0knLBSuo5q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b180e227",
        "outputId": "198f2bf3-c1b2-408c-b40e-1d670d658c1c"
      },
      "source": [
        "w2v_skipgram = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    sg=1 # Skip-gram architecture\n",
        ")\n",
        "\n",
        "print(\"Skip-gram Vocabulary size:\", len(w2v_skipgram.wv))"
      ],
      "id": "b180e227",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-gram Vocabulary size: 18095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f0da8ac",
        "outputId": "b2c12baa-4d9d-473c-e5dc-1dc9f7eced5b"
      },
      "source": [
        "print(\"Top 10 most similar words to 'space' (Skip-gram):\")\n",
        "for word, score in w2v_skipgram.wv.most_similar('space', topn=10):\n",
        "    print(f\"  {word:15s} {score:.3f}\")"
      ],
      "id": "3f0da8ac",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to 'space' (Skip-gram):\n",
            "  shuttle         0.748\n",
            "  nasa            0.707\n",
            "  exploration     0.677\n",
            "  jenks           0.669\n",
            "  redesign        0.667\n",
            "  station         0.663\n",
            "  telescope       0.652\n",
            "  ssto            0.651\n",
            "  goddard         0.650\n",
            "  mir             0.646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add bigrams using gensim.models.Phrases before training and observe changes."
      ],
      "metadata": {
        "id": "2ZRYj-9yvZup"
      },
      "id": "2ZRYj-9yvZup"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b4c9bd5",
        "outputId": "918d6896-c4ea-40cc-ab38-57fd6d7b5899"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "print(\"Imported Phrases and Phraser.\")"
      ],
      "id": "7b4c9bd5",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported Phrases and Phraser.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c581640",
        "outputId": "27297ef1-7d3d-40fb-f19f-a6c7c3f842f4"
      },
      "source": [
        "phrases = Phrases(corpus, min_count=5)\n",
        "bigram = Phraser(phrases)\n",
        "bigram_corpus = [bigram[doc] for doc in corpus]\n",
        "\n",
        "print(\"Example document from bigram_corpus (first 50 tokens):\")\n",
        "print(bigram_corpus[0][:50])"
      ],
      "id": "5c581640",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example document from bigram_corpus (first 50 tokens):\n",
            "['wondering_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports_car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc41043f",
        "outputId": "9aee7d5c-a338-41e1-b14c-3b6117101591"
      },
      "source": [
        "w2v_cbow_bigram = Word2Vec(\n",
        "    sentences=bigram_corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    sg=0 # CBOW architecture\n",
        ")\n",
        "\n",
        "print(\"Word2Vec CBOW with Bigrams Vocabulary size:\", len(w2v_cbow_bigram.wv))"
      ],
      "id": "dc41043f",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec CBOW with Bigrams Vocabulary size: 22884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4041ba1a",
        "outputId": "b23e58dd-f401-41e9-e66d-4ac0089e1039"
      },
      "source": [
        "ft_cbow_bigram = FastText(\n",
        "    sentences=bigram_corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    workers=4,\n",
        "    sg=0 # CBOW architecture\n",
        ")\n",
        "\n",
        "print(\"FastText CBOW with Bigrams Vocabulary size:\", len(ft_cbow_bigram.wv))"
      ],
      "id": "4041ba1a",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText CBOW with Bigrams Vocabulary size: 22884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0038ea02",
        "outputId": "9428519d-1e3f-436b-c69a-59be6e191145"
      },
      "source": [
        "print(\"Top 10 most similar words to 'space' (FastText CBOW with Bigrams):\")\n",
        "for word, score in ft_cbow_bigram.wv.most_similar('space', topn=10):\n",
        "    print(f\"  {word:15s} {score:.3f}\")"
      ],
      "id": "0038ea02",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to 'space' (FastText CBOW with Bigrams):\n",
            "  spacepac        0.988\n",
            "  spacehab        0.984\n",
            "  spaces          0.981\n",
            "  space_age       0.973\n",
            "  space_technology 0.970\n",
            "  space_agency    0.959\n",
            "  uk_serial       0.958\n",
            "  space_probes    0.954\n",
            "  space_launch    0.953\n",
            "  space_industry  0.952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35bcb83",
        "outputId": "b3c261cc-dd4b-4d6c-ed70-985604bfb686"
      },
      "source": [
        "print(\"Top 10 most similar words to 'space' (Word2Vec CBOW with Bigrams):\")\n",
        "for word, score in w2v_cbow_bigram.wv.most_similar('space', topn=10):\n",
        "    print(f\"  {word:15s} {score:.3f}\")"
      ],
      "id": "e35bcb83",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most similar words to 'space' (Word2Vec CBOW with Bigrams):\n",
            "  nasa            0.977\n",
            "  dec             0.972\n",
            "  ultrix          0.972\n",
            "  also_available  0.971\n",
            "  subject_get     0.970\n",
            "  available_anonymous 0.970\n",
            "  contact         0.970\n",
            "  includes        0.969\n",
            "  mit             0.969\n",
            "  available_via   0.969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ag01QQ1-tWG"
      },
      "id": "1Ag01QQ1-tWG",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8KC-890-uk_"
      },
      "id": "i8KC-890-uk_",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuHu9GSZ-ues"
      },
      "id": "WuHu9GSZ-ues",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nHaiwYPl-uXq"
      },
      "id": "nHaiwYPl-uXq",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVD7XhFn-uTC"
      },
      "id": "pVD7XhFn-uTC",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ToPAZJ9O-tMw"
      },
      "id": "ToPAZJ9O-tMw",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwYVyJ11-s9a"
      },
      "id": "gwYVyJ11-s9a",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HoEyIMuK4qM7"
      },
      "id": "HoEyIMuK4qM7",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e00ada6"
      },
      "source": [
        "# Task\n",
        "Train a Word2Vec model using the CBOW architecture (sg=0) with vector_size=100, window=5, min_count=5, and workers=4 on the preprocessed corpus. Then, identify and display the top 10 most similar words for 'space' using this CBOW model. Afterwards, re-train a Word2Vec model using the Skip-gram architecture (sg=1) with the same parameters (vector_size=100, window=5, min_count=5, workers=4) and find the top 10 most similar words for 'space' using this Skip-gram model. Finally, compare the nearest neighbors obtained from the CBOW and Skip-gram models for 'space', noting any observable differences in the semantic relationships captured by each model, and summarize the key differences and potential reasons."
      ],
      "id": "8e00ada6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "322a621c"
      },
      "source": [
        "## Train Word2Vec (CBOW)\n",
        "\n",
        "### Subtask:\n",
        "Train a Word2Vec model using the CBOW architecture (sg=0) with vector_size=100, window=5, min_count=5, and workers=4 on the preprocessed corpus.\n"
      ],
      "id": "322a621c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e249102b"
      },
      "source": [
        "**Reasoning**:\n",
        "To train a Word2Vec model with the CBOW architecture and print its vocabulary size, I will use the `Word2Vec` constructor with the specified parameters.\n",
        "\n"
      ],
      "id": "e249102b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4846fbb2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the CBOW model is trained, I will identify and display the top 10 most similar words for 'space' using the `most_similar` method of the `w2v_cbow.wv` object.\n",
        "\n"
      ],
      "id": "4846fbb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "089865b6"
      },
      "source": [
        "## Train Word2Vec (Skip-gram)\n",
        "\n",
        "### Subtask:\n",
        "Re-train a Word2Vec model using the Skip-gram architecture (sg=1) with `vector_size=100`, `window=5`, `min_count=5`, and `workers=4` on the preprocessed corpus. Then, identify and display the top 10 most similar words for 'space' using this Skip-gram model."
      ],
      "id": "089865b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c10fe67"
      },
      "source": [
        "**Reasoning**:\n",
        "To train a Word2Vec model with the Skip-gram architecture and the specified parameters, I will use the `Word2Vec` constructor with `sg=1`.\n",
        "\n"
      ],
      "id": "7c10fe67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c138b6dd"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Skip-gram model is trained, I will identify and display the top 10 most similar words for 'space' using the `most_similar` method of the `w2v_skipgram.wv` object.\n",
        "\n"
      ],
      "id": "c138b6dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba85303"
      },
      "source": [
        "## Compare CBOW and Skip-gram models\n",
        "\n",
        "### Comparison and summary:\n",
        "\n",
        "**CBOW (Continuous Bag-of-Words) for 'space':**\n",
        "- `nasa`, `shuttle`, `launch`, `aeronautics`, `research`, `station`, `commercial`, `center`, `satellite`, `development`\n",
        "- The neighbors appear to be highly relevant to the concept of 'outer space' and space exploration, with a focus on institutions, activities, and infrastructure related to space.\n",
        "\n",
        "**Skip-gram for 'space':**\n",
        "- `shuttle`, `nasa`, `exploration`, `jenks`, `redesign`, `station`, `telescope`, `ssto`, `goddard`, `mir`\n",
        "- While also relevant to 'outer space', this list includes more specific, potentially less common terms like `jenks`, `redesign`, `ssto`, `goddard`, and `mir`.\n",
        "\n",
        "**Key Differences and Potential Reasons:**\n",
        "\n",
        "1.  **Specificity of Neighbors:** Skip-gram tends to pick up on more specific or less frequent co-occurrence patterns, which might explain the presence of terms like `jenks` (likely a person's name or specific project), `redesign`, `ssto` (Single-Stage-To-Orbit, a technical term), `goddard` (Goddard Space Flight Center), and `mir` (space station). These words might appear in very close proximity to 'space' in the training corpus in specific contexts.\n",
        "\n",
        "2.  **Generality vs. Specificity of Context:**\n",
        "    *   **CBOW** (predicting the current word from context) often learns more general representations. It effectively averages the context words, leading to vectors that represent common meanings and broader associations for a given word. This results in neighbors that are more broadly related to the main topic (e.g., general space-related terms).\n",
        "    *   **Skip-gram** (predicting context words from the current word) tends to learn representations that capture more fine-grained semantic relationships. It's better at handling infrequent words and capturing specific contexts. This can lead to neighbors that are highly specific terms or entities that frequently co-occur with the target word, even if they are less common overall.\n",
        "\n",
        "3.  **Similarity Scores:** In this particular run, the similarity scores for CBOW neighbors are generally higher (e.g., `nasa` at 0.886) compared to Skip-gram (e.g., `shuttle` at 0.748). This is not a universal rule but can indicate that CBOW might be finding stronger, more common contextual relationships, or that its averaging approach leads to slightly less nuanced but more"
      ],
      "id": "8ba85303"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b994d96a"
      },
      "source": [
        "## Compare Results\n",
        "\n",
        "### Subtask:\n",
        "Compare the nearest neighbors obtained from the CBOW and Skip-gram models for the target word 'space', noting any observable differences in the semantic relationships captured by each model.\n"
      ],
      "id": "b994d96a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3528a56"
      },
      "source": [
        "## Compare Results\n",
        "\n",
        "### Subtask:\n",
        "Compare the nearest neighbors obtained from the CBOW and Skip-gram models for the target word 'space', noting any observable differences in the semantic relationships captured by each model.\n",
        "\n",
        "### Comparison for 'space':\n",
        "\n",
        "**CBOW Model (`w2v_cbow`):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (CBOW):\n",
        "  nasa            0.886\n",
        "  shuttle         0.880\n",
        "  launch          0.860\n",
        "  aeronautics     0.801\n",
        "  research        0.799\n",
        "  station         0.794\n",
        "  commercial      0.782\n",
        "  center          0.778\n",
        "  satellite       0.771\n",
        "  development     0.770\n",
        "```\n",
        "\n",
        "**Skip-gram Model (`w2v_skipgram`):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (Skip-gram):\n",
        "  shuttle         0.748\n",
        "  nasa            0.707\n",
        "  exploration     0.677\n",
        "  jenks           0.669\n",
        "  redesign        0.667\n",
        "  station         0.663\n",
        "  telescope       0.652\n",
        "  ssto            0.651\n",
        "  goddard         0.650\n",
        "  mir             0.646\n",
        "```\n",
        "\n",
        "**Key Differences and Observations:**\n",
        "\n",
        "1.  **Common words**: Both models identify 'nasa', 'shuttle', and 'station' as top neighbors, indicating these are strongly associated with 'space' regardless of the model architecture.\n",
        "2.  **CBOW's focus**: The CBOW model tends to group words that frequently appear *with* 'space' in common contexts. Its neighbors like 'launch', 'aeronautics', 'research', 'commercial', 'center', 'satellite', and 'development' are all strongly related to the *field* or *activities* associated with space, often appearing as modifiers or within the same phrase.\n",
        "3.  **Skip-gram's focus**: The Skip-gram model, which learns to predict context given a word, seems to capture a slightly more diverse set of related entities and concepts. While 'nasa', 'shuttle', and 'station' are present, words like 'exploration', 'telescope', 'ssto' (Single Stage To Orbit), 'goddard', and 'mir' (space station) suggest a focus on specific elements, missions, or tools *within* the broader 'space' domain, potentially highlighting more granular or distinct semantic relationships. The inclusion of 'jenks' and 'redesign' might indicate specific discussions within the 20 Newsgroups corpus that linked these terms to 'space' in a predictive manner.\n",
        "4.  **Similarity Scores**: The similarity scores for CBOW are generally higher than Skip-gram for its top neighbors. This could imply that CBOW finds these words more centrally related to 'space' in terms of shared contexts, or it might be a characteristic of how each model calculates similarity or how confidently they predict context vs. target words.\n",
        "\n",
        "In summary, both models capture relevant semantic information, but CBOW appears to favor words indicating general topical co-occurrence, while Skip-gram includes a broader range of specific entities and activities, potentially reflecting a more direct predictive relationship."
      ],
      "id": "f3528a56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0bcc00f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key differences observed between the CBOW and Skip-gram models based on their similarity results and discuss potential reasons for these differences.\n"
      ],
      "id": "a0bcc00f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e4954ad"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The key differences observed between the CBOW and Skip-gram models, based on their similarity results for the word 'space', are as follows:\n",
        "\n",
        "*   **Commonalities**: Both models identified 'nasa', 'shuttle', and 'station' among their top 10 most similar words, reinforcing their strong association with 'space' in the corpus.\n",
        "*   **Specificity of Neighbors**: The CBOW model's nearest neighbors ('launch', 'aeronautics', 'research', 'commercial', 'center', 'satellite', 'development') tend to be more general and abstract terms related to the broad context, activities, or domain of space. In contrast, the Skip-gram model's unique neighbors ('exploration', 'jenks', 'redesign', 'telescope', 'ssto', 'goddard', 'mir') are more specific entities, actions, or historical items within the space domain, including potentially less common terms specific to the dataset.\n",
        "*   **Similarity Scores**: The CBOW model generally produced higher similarity scores for its top neighbors (e.g., 'nasa' at 0.886) compared to the Skip-gram model (e.g., 'shuttle' at 0.748).\n",
        "\n",
        "These differences can be attributed to their distinct learning objectives:\n",
        "*   **CBOW (Continuous Bag-of-Words)**: Predicts the current word from its surrounding context. It tends to learn more general representations by averaging context words, leading to vectors that represent common meanings and broader associations based on frequent co-occurrence.\n",
        "*   **Skip-gram**: Predicts context words from a given target word. It often captures more fine-grained or specific semantic relationships and is better at handling infrequent words or capturing specific contexts. This can result in neighbors that are highly specific terms or entities that frequently co-occur with the target word, even if they are less common overall.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The CBOW model trained on the corpus yielded a vocabulary size of 18095 words.\n",
        "*   The top 10 most similar words to 'space' according to the CBOW model included 'nasa' (similarity: 0.886), 'shuttle' (0.880), 'launch' (0.860), 'aeronautics' (0.801), 'research' (0.799), 'station' (0.794), 'commercial' (0.782), 'center' (0.778), 'satellite' (0.771), and 'development' (0.770). These words generally represent broader concepts and activities related to space.\n",
        "*   The Skip-gram model also had a vocabulary size of 18095 words.\n",
        "*   The top 10 most similar words to 'space' according to the Skip-gram model were 'shuttle' (similarity: 0.748), 'nasa' (0.707), 'exploration' (0.677), 'jenks' (0.669), 'redesign' (0.667), 'station' (0.663), 'telescope' (0.652), 'ssto' (0.651), 'goddard' (0.650), and 'mir' (0.646). This list contains more specific entities and technical terms.\n",
        "*   'Nasa', 'shuttle', and 'station' were common neighbors across both models, highlighting their strong and consistent association with 'space'.\n",
        "*   CBOW's unique neighbors were generally related to the *field* or *activities* of space, while Skip-gram's unique neighbors included specific elements, missions, or tools, some of which might be artifacts of specific discussions within the 20 Newsgroups corpus (e.g., 'jenks', 'redesign').\n",
        "*   CBOW's similarity scores for its top neighbors were consistently higher than Skip-gram's, suggesting a stronger, more concentrated contextual relationship for CBOW, or a difference in how the models calculate similarity.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   When choosing between CBOW and Skip-gram, consider the nature of the desired semantic relationships: CBOW is better suited for capturing broader, more general contextual associations, while Skip-gram excels at identifying specific, fine-grained relationships and connections to less frequent terms.\n",
        "*   Further analysis could involve exploring the most similar words for other target words, including infrequent terms, to confirm these observed patterns in a broader context and validate the general tendencies of each model on this specific dataset.\n"
      ],
      "id": "8e4954ad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8648da76"
      },
      "source": [
        "# Task\n",
        "Generate bigrams from the preprocessed corpus using `gensim.models.Phrases`. Then, train a Word2Vec model and a FastText model using the CBOW architecture (sg=0) on the bigram-enriched corpus with `vector_size=100`, `window=5`, `min_count=5`, and `workers=4`. Finally, identify and display the top 10 most similar words for 'space' using both the Word2Vec and FastText models trained with bigrams, and summarize the observations regarding the impact of adding bigrams on the similarity results compared to previous results without bigrams."
      ],
      "id": "8648da76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082594f2"
      },
      "source": [
        "## Generate Bigrams\n",
        "\n",
        "### Subtask:\n",
        "Utilize `gensim.models.Phrases` to automatically detect and create common bigrams from the existing preprocessed corpus. This will effectively combine frequently co-occurring words into single tokens (e.g., 'new_york').\n"
      ],
      "id": "082594f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dd161ae"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate bigrams, I need to import the `Phrases` and `Phraser` classes from `gensim.models.phrases`.\n",
        "\n"
      ],
      "id": "2dd161ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dbd6601"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary classes are imported, I will instantiate the `Phrases` model to detect common bigrams in the `corpus`, create a `Phraser` object for efficient transformation, and then apply it to generate the `bigram_corpus`. Finally, I will print an example document to verify the bigram creation.\n",
        "\n"
      ],
      "id": "2dbd6601"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c6d4b1"
      },
      "source": [
        "**Reasoning**:\n",
        "I will train a Word2Vec model with the CBOW architecture on the `bigram_corpus` using the specified parameters, then display its vocabulary size.\n",
        "\n"
      ],
      "id": "38c6d4b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc52c99"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now train a FastText model using the CBOW architecture on the `bigram_corpus` with the specified parameters, then display its vocabulary size.\n",
        "\n"
      ],
      "id": "5bc52c99"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40315133"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the FastText CBOW model with bigrams is trained, I will identify and display the top 10 most similar words for 'space' using the `most_similar` method of the `ft_cbow_bigram.wv` object.\n",
        "\n"
      ],
      "id": "40315133"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1176734"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Word2Vec CBOW model with bigrams is trained, I will identify and display the top 10 most similar words for 'space' using the `most_similar` method of the `w2v_cbow_bigram.wv` object.\n",
        "\n"
      ],
      "id": "a1176734"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3695b22"
      },
      "source": [
        "## Summary: Impact of Bigrams on Similarity Results\n",
        "\n",
        "### Comparison of 'space' neighbors:\n",
        "\n",
        "**1. Word2Vec CBOW without Bigrams (from previous steps):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (CBOW):\n",
        "  nasa            0.886\n",
        "  shuttle         0.880\n",
        "  launch          0.860\n",
        "  aeronautics     0.801\n",
        "  research        0.799\n",
        "  station         0.794\n",
        "  commercial      0.782\n",
        "  center          0.778\n",
        "  satellite       0.771\n",
        "  development     0.770\n",
        "```\n",
        "\n",
        "**2. FastText CBOW without Bigrams (from previous steps):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (FastText CBOW):\n",
        "  spacehab        0.976\n",
        "  spacepac        0.975\n",
        "  airspace        0.960\n",
        "  pace            0.953\n",
        "  aerospace       0.949\n",
        "  spacewalk       0.925\n",
        "  spacecraft      0.918\n",
        "  spacelink       0.916\n",
        "  spaces          0.907\n",
        "  cyberspace      0.901\n",
        "```\n",
        "\n",
        "**3. Word2Vec CBOW with Bigrams (current output):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (Word2Vec CBOW with Bigrams):\n",
        "  nasa            0.977\n",
        "  dec             0.972\n",
        "  ultrix          0.972\n",
        "  also_available  0.971\n",
        "  subject_get     0.970\n",
        "  available_anonymous 0.970\n",
        "  contact         0.970\n",
        "  includes        0.969\n",
        "  mit             0.969\n",
        "  available_via   0.969\n",
        "```\n",
        "\n",
        "**4. FastText CBOW with Bigrams (current output):**\n",
        "```\n",
        "Top 10 most similar words to 'space' (FastText CBOW with Bigrams):\n",
        "  spacepac        0.988\n",
        "  spacehab        0.984\n",
        "  spaces          0.981\n",
        "  space_age       0.973\n",
        "  space_technology 0.970\n",
        "  space_agency    0.959\n",
        "  uk_serial       0.958\n",
        "  space_probes    0.954\n",
        "  space_launch    0.953\n",
        "  space_industry  0.952\n",
        "```\n",
        "\n",
        "### Observations and Impact of Bigrams:\n",
        "\n",
        "1.  **Increased Cohesion and Specificity (FastText with Bigrams):**\n",
        "    *   For FastText, adding bigrams significantly improved the relevance and specificity of the 'space' neighbors. Terms like `space_age`, `space_technology`, `space_agency`, `space_probes`, `space_launch`, and `space_industry` are clearly multi-word concepts directly related to 'space'. These were not present in the non-bigram FastText model's top neighbors, which focused more on morphological variants or words that *contain* \"space\" (e.g., `airspace`, `spacewalk`). This demonstrates that bigrams help capture phrases that function as a single semantic unit, leading to more meaningful and less lexically driven neighbors.\n",
        "\n",
        "2.  **Shift in Context (Word2Vec with Bigrams):**\n",
        "    *   For Word2Vec, the impact of bigrams is more drastic and perhaps unexpected. The original Word2Vec CBOW model for 'space' yielded highly relevant terms like `nasa`, `shuttle`, `launch`, `aeronautics`, etc. However, with bigrams, the similar words shifted dramatically to terms like `dec`, `ultrix`, `also_available`, `subject_get`, `mit`, etc. This suggests that the inclusion of bigrams changed the most dominant contexts for the word 'space' in the corpus, potentially due to the formation of many specific bigrams related to computing (`dec`, `ultrix`, `mit` often co-occur in computer-related discussions in 20 Newsgroups) that might have been less salient without explicitly combining them. The word 'space' itself might now be more associated with 'disk space' or 'address space' within a computing context rather than 'outer space' as the primary association.\n",
        "\n",
        "3.  **Vocabulary Expansion:** The vocabulary size for both models increased from 18095 (without bigrams) to 22884 (with bigrams), indicating that new composite tokens (bigrams) were successfully identified and added to the vocabulary. This enriches the model's understanding of multi-word expressions.\n",
        "\n",
        "4.  **Higher Similarity Scores:** In many cases, the similarity scores for relevant neighbors appear higher with bigrams (especially for FastText), suggesting that the models are more confident about the relatedness of these multi-word concepts.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Adding bigrams to the corpus for embedding training can profoundly influence the semantic relationships captured by the models. It helps in recognizing multi-word expressions as single entities, which can lead to more precise and contextually relevant neighbors, especially for models like FastText that can leverage subword information. However, the exact outcome depends on the nature of the corpus and the types of bigrams formed. In the case of Word2Vec with bigrams here, it seems to have highlighted a different dominant interpretation of 'space' within the 20 Newsgroups dataset, shifting from 'outer space' to more computer-related contexts."
      ],
      "id": "e3695b22"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a0b7e4"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the observations regarding the impact of adding bigrams on the similarity results for both Word2Vec and FastText models, comparing them to the previous results without bigrams.\n"
      ],
      "id": "12a0b7e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58785e33"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The impact of adding bigrams on the similarity results for both Word2Vec and FastText models, when compared to previous results without bigrams, shows distinct effects:\n",
        "\n",
        "*   **FastText with Bigrams:** Adding bigrams significantly improved the relevance and specificity of similar words for 'space'. The model captured multi-word concepts directly related to 'space', such as 'space\\_age', 'space\\_technology', and 'space\\_agency', which were absent in the non-bigram results. This indicates that bigrams helped FastText identify phrases that act as single semantic units, leading to more meaningful associations.\n",
        "*   **Word2Vec with Bigrams:** The impact on Word2Vec was more drastic, shifting the most similar words for 'space' from 'nasa', 'shuttle', and 'launch' to terms like 'dec', 'ultrix', 'also\\_available', and 'mit'. This suggests that bigram inclusion caused 'space' to be associated more with computing contexts (e.g., 'disk space', 'address space') rather than 'outer space' within the 20 Newsgroups corpus, highlighting a different dominant interpretation.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Bigrams were successfully generated from the preprocessed corpus, combining frequently co-occurring words into single tokens (e.g., 'wondering\\_anyone', 'sports\\_car', 'front\\_bumper').\n",
        "*   The vocabulary size increased from 18095 (without bigrams) to 22884 (with bigrams), reflecting the successful integration of new composite tokens into the models.\n",
        "*   **Word2Vec CBOW with Bigrams** showed a shift in the top 10 most similar words to 'space' from primarily space-exploration terms (e.g., 'nasa', 'shuttle', 'launch' with scores around 0.88) to computer-related terms (e.g., 'nasa' with 0.977, 'dec' with 0.972, 'ultrix' with 0.972, 'mit' with 0.969).\n",
        "*   **FastText CBOW with Bigrams** yielded highly relevant multi-word concepts for 'space', such as 'spacepac' (0.988), 'spacehab' (0.984), 'space\\_age' (0.973), 'space\\_technology' (0.970), and 'space\\_agency' (0.959), which were not present in its non-bigram counterpart's top results.\n",
        "*   Similarity scores for relevant neighbors generally appeared higher with bigrams, suggesting increased confidence in the relatedness of multi-word concepts, particularly for FastText.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Adding bigrams can significantly enhance the precision and contextual relevance of word embeddings, particularly for models like FastText that can leverage subword information to form meaningful multi-word concepts.\n",
        "*   The effect of bigrams is highly dependent on the corpus; for Word2Vec on the 20 Newsgroups dataset, bigrams shifted the primary interpretation of 'space' towards computing-related contexts, indicating the importance of understanding the dominant semantic fields created by bigrams.\n"
      ],
      "id": "58785e33"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aig230-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}